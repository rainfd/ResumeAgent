#!/usr/bin/env python3
"""
çˆ¬è™«è°ƒè¯•è„šæœ¬ - è¯Šæ–­å…·ä½“é—®é¢˜
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))

from resume_assistant.core.scraper import PlaywrightScraper
from resume_assistant.utils import get_logger

def debug_playwright_scraping():
    """è°ƒè¯•Playwrightçˆ¬å–è¿‡ç¨‹"""
    test_url = "https://www.zhipin.com/job_detail/29090ef211bc5eeb03Fz3dq0GFJT.html?securityId=bJzOfMuGJ2F22-61vKFksunOnKBpKMBvBxLCosBmd6oKgS-VKgAqEw7kDzXQdLe4VeIEz6q1iod1iRUkjA1tp9iHqBweo0KuhI9goLnJv-Rbe1Cs96KMmw~~&ka=personal_added_job_29090ef211bc5eeb03Fz3dq0GFJT"
    
    print("ğŸ” è°ƒè¯•Playwrightçˆ¬å–è¿‡ç¨‹")
    print(f"æµ‹è¯•URL: {test_url}")
    
    scraper = PlaywrightScraper()
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        scraper._setup_browser()
        page = scraper.context.new_page()
        
        print("\nğŸ“„ è®¿é—®é¡µé¢...")
        
        # æ³¨å…¥åæ£€æµ‹è„šæœ¬
        page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
            window.chrome = { runtime: {} };
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            Object.defineProperty(navigator, 'languages', {
                get: () => ['zh-CN', 'zh', 'en'],
            });
        """)
        
        # è®¿é—®é¡µé¢
        response = page.goto(test_url, wait_until='domcontentloaded', timeout=30000)
        print(f"HTTPçŠ¶æ€ç : {response.status}")
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        page.wait_for_timeout(5000)
        
        # è·å–é¡µé¢æ ‡é¢˜
        title = page.title()
        print(f"é¡µé¢æ ‡é¢˜: {title}")
        
        # æ£€æŸ¥é¡µé¢å†…å®¹
        body_text = page.inner_text('body') if page.query_selector('body') else ''
        print(f"é¡µé¢å†…å®¹é•¿åº¦: {len(body_text)}")
        print(f"é¡µé¢å†…å®¹é¢„è§ˆ: {body_text[:200]}...")
        
        # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
        blocked_keywords = ['éªŒè¯ç ', 'äººæœºéªŒè¯', 'è¯·ç™»å½•', 'è®¿é—®å—é™', 'æœºå™¨äºº']
        is_blocked = any(keyword in body_text for keyword in blocked_keywords)
        print(f"æ˜¯å¦è¢«æ‹¦æˆª: {is_blocked}")
        
        if is_blocked:
            for keyword in blocked_keywords:
                if keyword in body_text:
                    print(f"æ£€æµ‹åˆ°æ‹¦æˆªå…³é”®è¯: {keyword}")
        
        # ä¿å­˜é¡µé¢æˆªå›¾
        screenshot_path = "debug_screenshot.png"
        page.screenshot(path=screenshot_path)
        print(f"é¡µé¢æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
        
        # ä¿å­˜HTMLå†…å®¹
        html_content = page.content()
        with open("debug_page.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        print("é¡µé¢HTMLå·²ä¿å­˜: debug_page.html")
        
        # å°è¯•æŸ¥æ‰¾èŒä½ä¿¡æ¯å…ƒç´ 
        print("\nğŸ” æŸ¥æ‰¾èŒä½ä¿¡æ¯å…ƒç´ ...")
        
        selectors = {
            'title': ['.job-title', '.job-name', 'h1.name', 'h1'],
            'company': ['.company-name', '.info-company h3'],
            'salary': ['.salary', '.job-salary'],
            'location': ['.job-area', '.job-location'],
        }
        
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        for field, selector_list in selectors.items():
            print(f"\næŸ¥æ‰¾ {field}:")
            found = False
            for selector in selector_list:
                elements = soup.select(selector)
                if elements:
                    print(f"  é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ :")
                    for i, elem in enumerate(elements[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                        text = elem.get_text(strip=True)
                        print(f"    [{i+1}] {text[:50]}...")
                    found = True
                    break
            
            if not found:
                print(f"  æœªæ‰¾åˆ° {field} ä¿¡æ¯")
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•è¿‡ç¨‹å‡ºé”™: {e}")
    finally:
        scraper._cleanup_browser()

if __name__ == "__main__":
    debug_playwright_scraping()