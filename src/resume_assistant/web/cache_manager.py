"""Cache Management for Streamlit Web Interface."""

import streamlit as st
from typing import Any, Dict, List, Optional, Callable
from datetime import datetime, timedelta
import hashlib
import json

from ..utils import get_logger

logger = get_logger(__name__)

class CacheManager:
    """Streamlitç¼“å­˜ç®¡ç†å™¨"""
    
    @staticmethod
    def get_cache_key(prefix: str, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # å°†å‚æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶ç”Ÿæˆå“ˆå¸Œ
        data = {
            'args': args,
            'kwargs': kwargs,
            'timestamp': datetime.now().isoformat()[:19]  # ç²¾ç¡®åˆ°ç§’
        }
        content = json.dumps(data, sort_keys=True, default=str)
        hash_obj = hashlib.md5(content.encode())
        return f"{prefix}_{hash_obj.hexdigest()[:8]}"
    
    @staticmethod
    @st.cache_data(ttl=300)  # 5åˆ†é’Ÿç¼“å­˜
    def cache_job_scraping_result(url: str, scraper_func: Callable) -> Dict[str, Any]:
        """ç¼“å­˜èŒä½çˆ¬å–ç»“æœ"""
        try:
            logger.info(f"Cache miss for job scraping: {url}")
            return scraper_func(url)
        except Exception as e:
            logger.error(f"Cache job scraping error: {e}")
            return {}
    
    @staticmethod
    @st.cache_data(ttl=600)  # 10åˆ†é’Ÿç¼“å­˜
    def cache_resume_parsing_result(file_content: bytes, file_type: str, parser_func: Callable) -> Dict[str, Any]:
        """ç¼“å­˜ç®€å†è§£æç»“æœ"""
        try:
            # ä¸ºæ–‡ä»¶å†…å®¹ç”Ÿæˆå“ˆå¸Œä½œä¸ºç¼“å­˜é”®çš„ä¸€éƒ¨åˆ†
            content_hash = hashlib.md5(file_content).hexdigest()
            logger.info(f"Cache miss for resume parsing: {file_type}_{content_hash[:8]}")
            return parser_func(file_content, file_type)
        except Exception as e:
            logger.error(f"Cache resume parsing error: {e}")
            return {}
    
    @staticmethod
    @st.cache_data(ttl=1800)  # 30åˆ†é’Ÿç¼“å­˜
    def cache_ai_analysis_result(resume_id: str, job_id: str, analysis_func: Callable) -> Dict[str, Any]:
        """ç¼“å­˜AIåˆ†æç»“æœ"""
        try:
            cache_key = f"analysis_{resume_id}_{job_id}"
            logger.info(f"Cache miss for AI analysis: {cache_key}")
            return analysis_func()
        except Exception as e:
            logger.error(f"Cache AI analysis error: {e}")
            return {}
    
    @staticmethod
    @st.cache_data(ttl=900)  # 15åˆ†é’Ÿç¼“å­˜
    def cache_greeting_generation(job_data: str, resume_data: str, generation_func: Callable) -> List[str]:
        """ç¼“å­˜æ‰“æ‹›å‘¼è¯­ç”Ÿæˆç»“æœ"""
        try:
            # ä¸ºæ•°æ®ç”Ÿæˆå“ˆå¸Œ
            combined_data = f"{job_data}_{resume_data}"
            data_hash = hashlib.md5(combined_data.encode()).hexdigest()
            logger.info(f"Cache miss for greeting generation: {data_hash[:8]}")
            return generation_func()
        except Exception as e:
            logger.error(f"Cache greeting generation error: {e}")
            return []
    
    @staticmethod
    def clear_cache(cache_type: str = "all"):
        """æ¸…é™¤ç¼“å­˜"""
        try:
            if cache_type == "all":
                st.cache_data.clear()
                logger.info("All cache cleared")
            else:
                # è¿™é‡Œå¯ä»¥æ·»åŠ ç‰¹å®šç±»å‹çš„ç¼“å­˜æ¸…é™¤é€»è¾‘
                logger.info(f"Cache type {cache_type} clearing not implemented")
            
            # æ·»åŠ é€šçŸ¥
            if 'notifications' not in st.session_state:
                st.session_state.notifications = []
            
            st.session_state.notifications.append({
                'type': 'success',
                'message': f'ç¼“å­˜å·²æ¸…é™¤: {cache_type}',
                'timestamp': datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"Clear cache error: {e}")
            if 'notifications' not in st.session_state:
                st.session_state.notifications = []
            
            st.session_state.notifications.append({
                'type': 'error',
                'message': f'æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}',
                'timestamp': datetime.now().isoformat()
            })
    
    @staticmethod
    def get_cache_stats() -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # Streamlitç¼“å­˜ç»Ÿè®¡ï¼ˆè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿï¼Œå®é™…APIå¯èƒ½ä¸åŒï¼‰
            stats = {
                'total_cached_functions': 4,
                'cache_hit_rate': 0.75,  # æ¨¡æ‹Ÿ75%çš„å‘½ä¸­ç‡
                'total_cache_size': '2.3 MB',  # æ¨¡æ‹Ÿç¼“å­˜å¤§å°
                'last_cleared': st.session_state.get('last_cache_clear', 'æœªæ¸…é™¤'),
                'cache_enabled': True
            }
            return stats
        except Exception as e:
            logger.error(f"Get cache stats error: {e}")
            return {
                'error': str(e),
                'cache_enabled': False
            }
    
    @staticmethod
    def render_cache_management_panel():
        """æ¸²æŸ“ç¼“å­˜ç®¡ç†é¢æ¿"""
        st.subheader("ğŸ—„ï¸ ç¼“å­˜ç®¡ç†")
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        stats = CacheManager.get_cache_stats()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ç¼“å­˜å‡½æ•°æ•°é‡", stats.get('total_cached_functions', 0))
        
        with col2:
            hit_rate = stats.get('cache_hit_rate', 0)
            st.metric("å‘½ä¸­ç‡", f"{hit_rate * 100:.1f}%")
        
        with col3:
            st.metric("ç¼“å­˜å¤§å°", stats.get('total_cache_size', '0 MB'))
        
        # ç¼“å­˜æ§åˆ¶
        st.markdown("### ç¼“å­˜æ§åˆ¶")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ—‘ï¸ æ¸…é™¤å…¨éƒ¨ç¼“å­˜", type="secondary"):
                CacheManager.clear_cache("all")
                st.session_state.last_cache_clear = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                st.rerun()
        
        with col2:
            if st.button("ğŸ“Š åˆ·æ–°ç»Ÿè®¡", type="secondary"):
                st.rerun()
        
        with col3:
            cache_enabled = st.checkbox("å¯ç”¨ç¼“å­˜", value=stats.get('cache_enabled', True))
        
        # ç¼“å­˜è¯¦æƒ…
        with st.expander("ğŸ“‹ ç¼“å­˜è¯¦æƒ…"):
            st.json(stats)
    
    @staticmethod
    def setup_cache_config():
        """è®¾ç½®ç¼“å­˜é…ç½®"""
        # åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨æ­¤å‡½æ•°æ¥é…ç½®ç¼“å­˜
        # è¿™é‡Œå¯ä»¥è®¾ç½®å…¨å±€ç¼“å­˜å‚æ•°
        if 'cache_config' not in st.session_state:
            st.session_state.cache_config = {
                'job_scraping_ttl': 300,
                'resume_parsing_ttl': 600,
                'ai_analysis_ttl': 1800,
                'greeting_generation_ttl': 900,
                'max_cache_size': '50MB',
                'auto_clear_enabled': True,
                'auto_clear_interval': 3600  # 1å°æ—¶
            }
        
        logger.info("Cache configuration initialized")

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    @staticmethod
    def track_operation_time(operation_name: str):
        """è£…é¥°å™¨ï¼šè·Ÿè¸ªæ“ä½œæ—¶é—´"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = datetime.now()
                
                try:
                    result = func(*args, **kwargs)
                    
                    # è®°å½•æˆåŠŸæ“ä½œ
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()
                    
                    PerformanceMonitor._record_performance(operation_name, duration, True)
                    
                    return result
                    
                except Exception as e:
                    # è®°å½•å¤±è´¥æ“ä½œ
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()
                    
                    PerformanceMonitor._record_performance(operation_name, duration, False, str(e))
                    raise
            
            return wrapper
        return decorator
    
    @staticmethod
    def _record_performance(operation: str, duration: float, success: bool, error: str = None):
        """è®°å½•æ€§èƒ½æ•°æ®"""
        if 'performance_data' not in st.session_state:
            st.session_state.performance_data = []
        
        record = {
            'operation': operation,
            'duration': duration,
            'success': success,
            'error': error,
            'timestamp': datetime.now().isoformat()
        }
        
        st.session_state.performance_data.append(record)
        
        # åªä¿ç•™æœ€è¿‘100æ¡è®°å½•
        if len(st.session_state.performance_data) > 100:
            st.session_state.performance_data = st.session_state.performance_data[-100:]
        
        logger.info(f"Performance recorded: {operation} - {duration:.2f}s - {'Success' if success else 'Failed'}")
    
    @staticmethod
    def get_performance_summary() -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if 'performance_data' not in st.session_state:
            return {'total_operations': 0}
        
        data = st.session_state.performance_data
        
        if not data:
            return {'total_operations': 0}
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_ops = len(data)
        success_ops = sum(1 for record in data if record['success'])
        success_rate = success_ops / total_ops if total_ops > 0 else 0
        
        durations = [record['duration'] for record in data if record['success']]
        avg_duration = sum(durations) / len(durations) if durations else 0
        
        # æŒ‰æ“ä½œç±»å‹åˆ†ç»„
        operations = {}
        for record in data:
            op_name = record['operation']
            if op_name not in operations:
                operations[op_name] = {'count': 0, 'avg_duration': 0, 'success_rate': 0}
            
            operations[op_name]['count'] += 1
        
        return {
            'total_operations': total_ops,
            'success_rate': success_rate,
            'average_duration': avg_duration,
            'operations_breakdown': operations,
            'recent_errors': [record for record in data[-10:] if not record['success']]
        }
    
    @staticmethod
    def render_performance_panel():
        """æ¸²æŸ“æ€§èƒ½ç›‘æ§é¢æ¿"""
        st.subheader("ğŸ“ˆ æ€§èƒ½ç›‘æ§")
        
        summary = PerformanceMonitor.get_performance_summary()
        
        if summary['total_operations'] == 0:
            st.info("è¿˜æ²¡æœ‰æ€§èƒ½æ•°æ®è®°å½•")
            return
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("æ€»æ“ä½œæ•°", summary['total_operations'])
        
        with col2:
            success_rate = summary['success_rate'] * 100
            st.metric("æˆåŠŸç‡", f"{success_rate:.1f}%")
        
        with col3:
            avg_duration = summary['average_duration']
            st.metric("å¹³å‡è€—æ—¶", f"{avg_duration:.2f}s")
        
        # æ“ä½œåˆ†è§£
        if summary['operations_breakdown']:
            st.markdown("### æ“ä½œåˆ†è§£")
            
            import pandas as pd
            ops_data = []
            for op_name, op_data in summary['operations_breakdown'].items():
                ops_data.append({
                    'æ“ä½œ': op_name,
                    'æ¬¡æ•°': op_data['count'],
                    'å¹³å‡è€—æ—¶': f"{op_data.get('avg_duration', 0):.2f}s"
                })
            
            df = pd.DataFrame(ops_data)
            st.dataframe(df, use_container_width=True)
        
        # æœ€è¿‘é”™è¯¯
        recent_errors = summary.get('recent_errors', [])
        if recent_errors:
            with st.expander(f"âš ï¸ æœ€è¿‘é”™è¯¯ ({len(recent_errors)}æ¡)"):
                for error in recent_errors:
                    st.error(f"**{error['operation']}**: {error['error']} ({error['timestamp'][:19]})")