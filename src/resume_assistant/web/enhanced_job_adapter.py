"""å¢å¼ºçš„èŒä½ç®¡ç†Webé€‚é…å™¨

é›†æˆäº†æ–°çš„çˆ¬è™«åè°ƒå™¨ï¼Œæä¾›æ›´å¼ºå¤§çš„å¤šç«™ç‚¹æ”¯æŒå’Œæ€§èƒ½ç›‘æ§åŠŸèƒ½ã€‚
"""

import asyncio
import time
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime
import streamlit as st

from ..core.scraping_orchestrator import (
    ScrapingOrchestrator, 
    ScrapingConfig, 
    ScrapingResult,
    SiteSupport,
    get_scraping_config
)
from ..data.models import JobInfo
from ..data.database import DatabaseManager
from ..utils import get_logger

logger = get_logger(__name__)


class EnhancedJobAdapter:
    """å¢å¼ºçš„èŒä½ç®¡ç†é€‚é…å™¨"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.logger = get_logger(self.__class__.__name__)
        self.orchestrator: Optional[ScrapingOrchestrator] = None
        self._current_config = None
        
    def _get_orchestrator(self, config: Optional[ScrapingConfig] = None) -> ScrapingOrchestrator:
        """è·å–æˆ–åˆ›å»ºçˆ¬è™«åè°ƒå™¨å®ä¾‹"""
        # å¦‚æœé…ç½®æœ‰å˜åŒ–ï¼Œé‡æ–°åˆ›å»ºåè°ƒå™¨
        if config != self._current_config or not self.orchestrator:
            if self.orchestrator:
                self.orchestrator.cleanup()
            
            self.orchestrator = ScrapingOrchestrator(config or ScrapingConfig())
            self._current_config = config
            
        return self.orchestrator
    
    async def scrape_single_job_async(
        self, 
        url: str, 
        config: Optional[ScrapingConfig] = None,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> ScrapingResult:
        """å¼‚æ­¥çˆ¬å–å•ä¸ªèŒä½"""
        orchestrator = self._get_orchestrator(config)
        
        if progress_callback:
            progress_callback("å¼€å§‹çˆ¬å–èŒä½ä¿¡æ¯...")
        
        try:
            # æ£€æŸ¥URLæ˜¯å¦è¢«æ”¯æŒ
            if not orchestrator.is_url_supported(url):
                return ScrapingResult(
                    success=False,
                    error="ä¸æ”¯æŒçš„æ‹›è˜ç½‘ç«™URL",
                    url=url,
                    scraped_at=datetime.now()
                )
            
            if progress_callback:
                progress_callback("æ­£åœ¨è¿æ¥ç›®æ ‡ç½‘ç«™...")
            
            # æ‰§è¡Œçˆ¬å–
            result = await orchestrator.scrape_single_job(url)
            
            if result.success and result.job:
                if progress_callback:
                    progress_callback("æ­£åœ¨ä¿å­˜åˆ°æ•°æ®åº“...")
                
                # è½¬æ¢ä¸ºJobInfoå¹¶ä¿å­˜
                job_info = self._convert_to_job_info(result.job, url)
                job_id = await self.db_manager.save_job(job_info)
                
                if progress_callback:
                    progress_callback(f"æˆåŠŸä¿å­˜èŒä½: {result.job.title}")
                
                self.logger.info(f"æˆåŠŸçˆ¬å–å¹¶ä¿å­˜èŒä½: {result.job.title} (ID: {job_id})")
                
            return result
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            return ScrapingResult(
                success=False,
                error=f"çˆ¬å–è¿‡ç¨‹å‡ºé”™: {str(e)}",
                url=url,
                scraped_at=datetime.now()
            )
    
    def scrape_single_job_sync(
        self, 
        url: str, 
        config: Optional[Dict[str, Any]] = None,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> ScrapingResult:
        """åŒæ­¥çˆ¬å–å•ä¸ªèŒä½ï¼ˆStreamlitå…¼å®¹ï¼‰"""
        # è½¬æ¢é…ç½®
        scraping_config = None
        if config:
            scraping_config = get_scraping_config(**config)
        
        # è¿è¡Œå¼‚æ­¥å‡½æ•°
        loop = None
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œä½¿ç”¨çº¿ç¨‹æ± 
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    asyncio.run,
                    self.scrape_single_job_async(url, scraping_config, progress_callback)
                )
                return future.result()
        else:
            return loop.run_until_complete(
                self.scrape_single_job_async(url, scraping_config, progress_callback)
            )
    
    async def scrape_multiple_jobs_async(
        self, 
        urls: List[str], 
        config: Optional[ScrapingConfig] = None,
        progress_callback: Optional[Callable[[str, float], None]] = None
    ) -> List[ScrapingResult]:
        """å¼‚æ­¥æ‰¹é‡çˆ¬å–èŒä½"""
        orchestrator = self._get_orchestrator(config)
        
        if progress_callback:
            progress_callback(f"å¼€å§‹æ‰¹é‡çˆ¬å– {len(urls)} ä¸ªèŒä½...", 0.0)
        
        try:
            # è¿‡æ»¤æ”¯æŒçš„URL
            supported_urls = []
            unsupported_results = []
            
            for url in urls:
                if orchestrator.is_url_supported(url):
                    supported_urls.append(url)
                else:
                    unsupported_results.append(ScrapingResult(
                        success=False,
                        error="ä¸æ”¯æŒçš„æ‹›è˜ç½‘ç«™URL",
                        url=url,
                        scraped_at=datetime.now()
                    ))
            
            if progress_callback and unsupported_results:
                progress_callback(f"è¿‡æ»¤æ‰ {len(unsupported_results)} ä¸ªä¸æ”¯æŒçš„URL", 0.1)
            
            # æ‰§è¡Œæ‰¹é‡çˆ¬å–
            if supported_urls:
                if progress_callback:
                    progress_callback("æ­£åœ¨å¹¶å‘çˆ¬å–æ”¯æŒçš„URL...", 0.2)
                
                scraping_results = await orchestrator.scrape_multiple_jobs(supported_urls)
                
                # ä¿å­˜æˆåŠŸçš„ç»“æœåˆ°æ•°æ®åº“
                saved_count = 0
                for i, result in enumerate(scraping_results):
                    if result.success and result.job:
                        try:
                            job_info = self._convert_to_job_info(result.job, result.url)
                            await self.db_manager.save_job(job_info)
                            saved_count += 1
                        except Exception as e:
                            self.logger.error(f"ä¿å­˜èŒä½å¤±è´¥: {e}")
                            result.success = False
                            result.error = f"ä¿å­˜å¤±è´¥: {str(e)}"
                    
                    if progress_callback:
                        progress = 0.2 + 0.7 * (i + 1) / len(scraping_results)
                        progress_callback(f"å·²å¤„ç† {i + 1}/{len(scraping_results)} ä¸ªURL", progress)
                
                if progress_callback:
                    progress_callback(f"å®Œæˆï¼æˆåŠŸä¿å­˜ {saved_count} ä¸ªèŒä½", 1.0)
                
                # åˆå¹¶ç»“æœ
                all_results = scraping_results + unsupported_results
                
            else:
                all_results = unsupported_results
                if progress_callback:
                    progress_callback("æ²¡æœ‰æ”¯æŒçš„URLéœ€è¦å¤„ç†", 1.0)
            
            return all_results
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
            # è¿”å›é”™è¯¯ç»“æœ
            return [ScrapingResult(
                success=False,
                error=f"æ‰¹é‡çˆ¬å–å‡ºé”™: {str(e)}",
                url=url,
                scraped_at=datetime.now()
            ) for url in urls]
    
    def scrape_multiple_jobs_sync(
        self, 
        urls: List[str], 
        config: Optional[Dict[str, Any]] = None,
        progress_callback: Optional[Callable[[str, float], None]] = None
    ) -> List[ScrapingResult]:
        """åŒæ­¥æ‰¹é‡çˆ¬å–èŒä½ï¼ˆStreamlitå…¼å®¹ï¼‰"""
        # è½¬æ¢é…ç½®
        scraping_config = None
        if config:
            scraping_config = get_scraping_config(**config)
        
        # è¿è¡Œå¼‚æ­¥å‡½æ•°
        loop = None
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œä½¿ç”¨çº¿ç¨‹æ± 
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    asyncio.run,
                    self.scrape_multiple_jobs_async(urls, scraping_config, progress_callback)
                )
                return future.result()
        else:
            return loop.run_until_complete(
                self.scrape_multiple_jobs_async(urls, scraping_config, progress_callback)
            )
    
    def _convert_to_job_info(self, job: Any, url: str) -> JobInfo:
        """å°†çˆ¬å–çš„Jobå¯¹è±¡è½¬æ¢ä¸ºJobInfoæ•°æ®æ¨¡å‹"""
        return JobInfo(
            url=url,
            title=getattr(job, 'title', ''),
            company=getattr(job, 'company', ''),
            description=getattr(job, 'description', ''),
            requirements=getattr(job, 'requirements', ''),
            skills=getattr(job, 'skills', []),
            location=getattr(job, 'location', ''),
            salary_range=getattr(job, 'salary_range', ''),
            salary_min=getattr(job, 'salary_min', None),
            salary_max=getattr(job, 'salary_max', None),
            experience_min=getattr(job, 'experience_min', None),
            experience_max=getattr(job, 'experience_max', None),
            education=getattr(job, 'education', ''),
            company_info=getattr(job, 'company_info', ''),
            published_time=getattr(job, 'published_time', ''),
            source=getattr(job, 'source', 'unknown'),
            crawled_at=datetime.now()
        )
    
    def get_supported_sites(self) -> List[Dict[str, str]]:
        """è·å–æ”¯æŒçš„æ‹›è˜ç½‘ç«™åˆ—è¡¨"""
        orchestrator = self._get_orchestrator()
        sites_info = []
        
        for site in SiteSupport:
            site_info = {
                'value': site.value,
                'name': self._get_site_display_name(site.value),
                'example_url': self._get_site_example_url(site.value),
                'status': 'available' if site.value in orchestrator.multi_scraper.scrapers else 'unavailable'
            }
            sites_info.append(site_info)
        
        return sites_info
    
    def _get_site_display_name(self, site_value: str) -> str:
        """è·å–ç½‘ç«™æ˜¾ç¤ºåç§°"""
        name_mapping = {
            'boss': 'BOSSç›´è˜',
            'lagou': 'æ‹‰å‹¾ç½‘',
            'zhilian': 'æ™ºè”æ‹›è˜',
            'liepin': 'çŒè˜ç½‘',
            '51job': 'å‰ç¨‹æ— å¿§'
        }
        return name_mapping.get(site_value, site_value.upper())
    
    def _get_site_example_url(self, site_value: str) -> str:
        """è·å–ç½‘ç«™ç¤ºä¾‹URL"""
        example_mapping = {
            'boss': 'https://www.zhipin.com/job_detail/123456.html',
            'lagou': 'https://www.lagou.com/jobs/123456.html',
            'zhilian': 'https://jobs.zhaopin.com/123456.htm',
            'liepin': 'https://www.liepin.com/job/123456.shtml',
            '51job': 'https://jobs.51job.com/123456.html'
        }
        return example_mapping.get(site_value, '')
    
    def is_url_supported(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦è¢«æ”¯æŒ"""
        orchestrator = self._get_orchestrator()
        return orchestrator.is_url_supported(url)
    
    def get_performance_stats(self) -> Optional[Dict[str, Any]]:
        """è·å–çˆ¬è™«æ€§èƒ½ç»Ÿè®¡"""
        if self.orchestrator:
            return self.orchestrator.get_performance_stats()
        return None
    
    async def health_check_async(self) -> Dict[str, Any]:
        """å¼‚æ­¥å¥åº·æ£€æŸ¥"""
        orchestrator = self._get_orchestrator()
        return await orchestrator.health_check()
    
    def health_check_sync(self) -> Dict[str, Any]:
        """åŒæ­¥å¥åº·æ£€æŸ¥ï¼ˆStreamlitå…¼å®¹ï¼‰"""
        loop = None
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        if loop.is_running():
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, self.health_check_async())
                return future.result()
        else:
            return loop.run_until_complete(self.health_check_async())
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.orchestrator:
            self.orchestrator.cleanup()
            self.orchestrator = None
            self._current_config = None


# Streamlit é›†æˆåŠ©æ‰‹å‡½æ•°
def create_scraping_config_from_ui() -> ScrapingConfig:
    """ä»Streamlit UIåˆ›å»ºçˆ¬è™«é…ç½®"""
    with st.expander("ğŸ”§ çˆ¬è™«é…ç½®", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            headless = st.checkbox("æ— å¤´æ¨¡å¼", value=False, help="å¼€å¯åæµè§ˆå™¨åœ¨åå°è¿è¡Œï¼Œå…³é—­åå¯ä»¥çœ‹åˆ°æµè§ˆå™¨çª—å£")
            max_retries = st.number_input("æœ€å¤§é‡è¯•æ¬¡æ•°", min_value=1, max_value=10, value=3)
            concurrent_limit = st.number_input("å¹¶å‘é™åˆ¶", min_value=1, max_value=10, value=3)
        
        with col2:
            timeout = st.number_input("è¶…æ—¶æ—¶é—´(ç§’)", min_value=10, max_value=120, value=30)
            enable_monitoring = st.checkbox("æ€§èƒ½ç›‘æ§", value=True, help="è®°å½•çˆ¬å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯")
            data_validation = st.checkbox("æ•°æ®éªŒè¯", value=True, help="å¯¹çˆ¬å–çš„æ•°æ®è¿›è¡Œè´¨é‡æ£€æŸ¥")
    
    return ScrapingConfig(
        headless=headless,
        max_retries=max_retries,
        concurrent_limit=concurrent_limit,
        timeout=timeout,
        enable_monitoring=enable_monitoring,
        data_validation=data_validation
    )


def display_scraping_result(result: ScrapingResult):
    """æ˜¾ç¤ºçˆ¬å–ç»“æœ"""
    if result.success and result.job:
        st.success(f"âœ… æˆåŠŸçˆ¬å–: {result.job.title}")
        
        with st.expander("æŸ¥çœ‹èŒä½è¯¦æƒ…", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**å…¬å¸**: {result.job.company}")
                st.write(f"**åœ°ç‚¹**: {getattr(result.job, 'location', 'æœªçŸ¥')}")
                st.write(f"**è–ªèµ„**: {getattr(result.job, 'salary_range', 'é¢è®®')}")
            
            with col2:
                st.write(f"**ç»éªŒ**: {getattr(result.job, 'experience_min', 0)}-{getattr(result.job, 'experience_max', 0)}å¹´")
                st.write(f"**å­¦å†**: {getattr(result.job, 'education', 'ä¸é™')}")
                st.write(f"**æ¥æº**: {getattr(result.job, 'source', 'unknown')}")
            
            if hasattr(result.job, 'skills') and result.job.skills:
                st.write("**æŠ€èƒ½è¦æ±‚**:")
                skills_text = " | ".join(result.job.skills[:10])  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                st.text(skills_text)
            
            if hasattr(result.job, 'description') and result.job.description:
                st.write("**èŒä½æè¿°**:")
                st.text_area("", value=result.job.description[:500] + "..." if len(result.job.description) > 500 else result.job.description, height=150, disabled=True)
    
    else:
        st.error(f"âŒ çˆ¬å–å¤±è´¥: {result.error}")


def display_batch_results(results: List[ScrapingResult]):
    """æ˜¾ç¤ºæ‰¹é‡çˆ¬å–ç»“æœ"""
    if not results:
        st.warning("æ²¡æœ‰çˆ¬å–ç»“æœ")
        return
    
    successful = [r for r in results if r.success]
    failed = [r for r in results if not r.success]
    
    # ç»Ÿè®¡ä¿¡æ¯
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("æ€»æ•°", len(results))
    
    with col2:
        st.metric("æˆåŠŸ", len(successful), delta=f"{len(successful)/len(results)*100:.1f}%")
    
    with col3:
        st.metric("å¤±è´¥", len(failed), delta=f"{len(failed)/len(results)*100:.1f}%" if failed else None)
    
    # æˆåŠŸç»“æœ
    if successful:
        st.subheader("âœ… æˆåŠŸçˆ¬å–çš„èŒä½")
        for result in successful:
            with st.expander(f"{result.job.title} - {result.job.company}"):
                display_scraping_result(result)
    
    # å¤±è´¥ç»“æœ
    if failed:
        st.subheader("âŒ çˆ¬å–å¤±è´¥çš„URL")
        for result in failed:
            st.error(f"{result.url}: {result.error}")


def display_performance_stats(stats: Optional[Dict[str, Any]]):
    """æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡"""
    if not stats:
        st.info("æš‚æ— æ€§èƒ½ç»Ÿè®¡æ•°æ®")
        return
    
    st.subheader("ğŸ“Š çˆ¬è™«æ€§èƒ½ç»Ÿè®¡")
    
    # æ€»ä½“ç»Ÿè®¡
    overall = stats.get('overall', {})
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æ€»å°è¯•æ¬¡æ•°", overall.get('total_attempts', 0))
    
    with col2:
        st.metric("æˆåŠŸç‡", overall.get('success_rate', '0%'))
    
    with col3:
        st.metric("å¹³å‡å“åº”æ—¶é—´", overall.get('average_response_time', '0s'))
    
    with col4:
        st.metric("æˆåŠŸæ¬¡æ•°", overall.get('successful_scrapes', 0))
    
    # æŒ‰ç½‘ç«™ç»Ÿè®¡
    by_site = stats.get('by_site', {})
    if by_site:
        st.subheader("æŒ‰ç½‘ç«™ç»Ÿè®¡")
        
        for site, site_stats in by_site.items():
            with st.expander(f"ğŸ“ˆ {site.upper()} ç»Ÿè®¡"):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("å°è¯•æ¬¡æ•°", site_stats.get('attempts', 0))
                
                with col2:
                    st.metric("æˆåŠŸç‡", site_stats.get('success_rate', '0%'))
                
                with col3:
                    st.metric("å¹³å‡å“åº”æ—¶é—´", site_stats.get('avg_response_time', '0s'))
                
                if site_stats.get('last_success'):
                    st.text(f"æœ€åæˆåŠŸ: {site_stats['last_success']}")
                
                if site_stats.get('last_failure'):
                    st.text(f"æœ€åå¤±è´¥: {site_stats['last_failure']}")