"""æ‹‰å‹¾ç½‘èŒä½çˆ¬è™«

ä¸“é—¨ç”¨äºçˆ¬å–æ‹‰å‹¾ç½‘(lagou.com)çš„èŒä½ä¿¡æ¯ï¼ŒåŒ…å«é’ˆå¯¹è¯¥ç½‘ç«™ç‰¹æ€§çš„ä¼˜åŒ–ã€‚
"""

import re
import time
import json
import random
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse, parse_qs
from datetime import datetime

try:
    import requests
    from bs4 import BeautifulSoup
    HAS_SCRAPING_SUPPORT = True
except ImportError:
    HAS_SCRAPING_SUPPORT = False

try:
    from playwright.sync_api import sync_playwright
    HAS_PLAYWRIGHT_SUPPORT = True
except ImportError:
    HAS_PLAYWRIGHT_SUPPORT = False

from ..utils import get_logger
from ..utils.errors import NetworkError, ResumeAssistantError
from .scraper import JobScraper, ScrapingResult
from .job_manager import Job

logger = get_logger(__name__)


class LagouScraper(JobScraper):
    """æ‹‰å‹¾ç½‘èŒä½çˆ¬è™«"""
    
    def __init__(self, headless: bool = False):
        """åˆå§‹åŒ–æ‹‰å‹¾ç½‘çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        """
        if not HAS_PLAYWRIGHT_SUPPORT:
            raise ResumeAssistantError("Playwrightä¾èµ–åº“æœªå®‰è£…ã€‚è¯·å®‰è£…: pip install playwright")
        
        self.logger = get_logger(self.__class__.__name__)
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None
        self.headless = headless
        
        # æ‹‰å‹¾ç½‘ç‰¹æœ‰çš„è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1'
        }
    
    def _setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨"""
        if not self.playwright:
            self.playwright = sync_playwright().start()
        
        # æµè§ˆå™¨å¯åŠ¨å‚æ•°
        launch_args = [
            '--no-sandbox',
            '--disable-blink-features=AutomationControlled',
            '--disable-web-security',
            '--disable-dev-shm-usage',
            '--disable-extensions',
            '--no-first-run',
            '--no-default-browser-check',
            '--disable-default-apps',
            '--disable-popup-blocking',
            '--disable-translate'
        ]
        
        if not self.headless:
            self.logger.info("å¯åŠ¨æœ‰å¤´æµè§ˆå™¨æ¨¡å¼ï¼ˆæ‹‰å‹¾ç½‘ä¸“ç”¨ï¼‰")
        
        # åˆ›å»ºæµè§ˆå™¨
        self.browser = self.playwright.chromium.launch(
            headless=self.headless,
            args=launch_args
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        self.context = self.browser.new_context(
            user_agent=self.headers['User-Agent'],
            viewport={'width': 1920, 'height': 1080},
            extra_http_headers=self.headers
        )
        
        # åˆ›å»ºé¡µé¢
        self.page = self.context.new_page()
        
        # æ‹‰å‹¾ç½‘ç‰¹æœ‰çš„åæ£€æµ‹è®¾ç½®
        self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            });
            
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            
            Object.defineProperty(navigator, 'languages', {
                get: () => ['zh-CN', 'zh', 'en'],
            });
            
            window.chrome = {
                runtime: {},
            };
            
            Object.defineProperty(navigator, 'permissions', {
                get: () => ({
                    query: () => Promise.resolve({ state: 'granted' }),
                }),
            });
        """)
    
    def _validate_lagou_url(self, url: str) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ‹‰å‹¾ç½‘èŒä½URL"""
        try:
            parsed = urlparse(url)
            return 'lagou.com' in parsed.netloc.lower() and '/jobs/' in parsed.path
        except Exception:
            return False
    
    def _extract_job_id(self, url: str) -> Optional[str]:
        """ä»URLä¸­æå–èŒä½ID"""
        try:
            # æ‹‰å‹¾ç½‘èŒä½URLæ ¼å¼: https://www.lagou.com/jobs/123456.html
            match = re.search(r'/jobs/(\d+)\.html', url)
            if match:
                return match.group(1)
                
            # APIæ ¼å¼: https://www.lagou.com/jobs/123456
            match = re.search(r'/jobs/(\d+)$', url)
            if match:
                return match.group(1)
                
        except Exception as e:
            self.logger.error(f"æå–èŒä½IDå¤±è´¥: {e}")
        
        return None
    
    def _simulate_human_behavior(self):
        """æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º"""
        try:
            # éšæœºæ»šåŠ¨
            scroll_count = random.randint(2, 4)
            for _ in range(scroll_count):
                scroll_distance = random.randint(300, 800)
                self.page.evaluate(f"window.scrollBy(0, {scroll_distance})")
                time.sleep(random.uniform(0.5, 1.5))
            
            # éšæœºé¼ æ ‡ç§»åŠ¨
            for _ in range(random.randint(2, 4)):
                x = random.randint(100, 1000)
                y = random.randint(100, 600)
                self.page.mouse.move(x, y)
                time.sleep(random.uniform(0.1, 0.3))
            
            # æ¨¡æ‹Ÿé˜…è¯»æ—¶é—´
            time.sleep(random.uniform(2.0, 5.0))
            
        except Exception as e:
            self.logger.warning(f"æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ—¶å‡ºé”™: {e}")
    
    def _check_anti_robot(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è§¦å‘äº†åçˆ¬éªŒè¯"""
        anti_robot_indicators = [
            'è¯·è¾“å…¥éªŒè¯ç ',
            'äººæœºéªŒè¯',
            'å®‰å…¨éªŒè¯',
            'robot',
            'captcha',
            'è®¿é—®è¿‡äºé¢‘ç¹',
            'è¯·ç¨åå†è¯•'
        ]
        
        content_lower = content.lower()
        for indicator in anti_robot_indicators:
            if indicator.lower() in content_lower:
                return True
        
        return False
    
    def _wait_for_manual_verification(self):
        """ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å¤„ç†éªŒè¯"""
        print("\n" + "="*50)
        print("ğŸš¨ æ‹‰å‹¾ç½‘æ£€æµ‹åˆ°éœ€è¦äººå·¥éªŒè¯ï¼")
        print("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆä»¥ä¸‹æ“ä½œï¼š")
        print("1. å®ŒæˆäººæœºéªŒè¯/éªŒè¯ç ")
        print("2. å¦‚éœ€ç™»å½•ï¼Œè¯·å®Œæˆç™»å½•")
        print("3. ç¡®ä¿é¡µé¢æ­£å¸¸æ˜¾ç¤ºèŒä½ä¿¡æ¯")
        print("4. å®Œæˆåè¯·åœ¨æ­¤å¤„æŒ‰ Enter ç»§ç»­...")
        print("="*50)
        
        input("ç­‰å¾…æ‰‹åŠ¨éªŒè¯å®Œæˆï¼ŒæŒ‰ Enter ç»§ç»­...")
        
        # ç»™ç”¨æˆ·ä¸€äº›æ—¶é—´å®Œæˆæ“ä½œ
        time.sleep(2)
    
    def _extract_job_info(self, job_id: str) -> Optional[Job]:
        """ä»é¡µé¢æå–èŒä½ä¿¡æ¯"""
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            self.page.wait_for_load_state('networkidle', timeout=30000)
            
            # è·å–é¡µé¢å†…å®¹
            content = self.page.content()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯
            if self._check_anti_robot(content):
                self.logger.warning("æ£€æµ‹åˆ°åçˆ¬éªŒè¯ï¼Œéœ€è¦äººå·¥å¤„ç†")
                self._wait_for_manual_verification()
                content = self.page.content()
            
            soup = BeautifulSoup(content, 'html.parser')
            
            # æå–èŒä½ä¿¡æ¯
            job = Job()
            
            # èŒä½æ ‡é¢˜
            title_elem = soup.find('span', class_='name') or soup.find('h1', class_='position-head-wrap-name')
            if title_elem:
                job.title = title_elem.get_text(strip=True)
            
            # å…¬å¸åç§°
            company_elem = soup.find('a', class_='b2') or soup.find('h2', class_='fl')
            if company_elem:
                job.company = company_elem.get_text(strip=True)
            
            # è–ªèµ„ä¿¡æ¯
            salary_elem = soup.find('span', class_='salary') or soup.find('span', class_='position-head-wrap-salary')
            if salary_elem:
                salary_text = salary_elem.get_text(strip=True)
                job.salary_range = salary_text
                
                # è§£æè–ªèµ„èŒƒå›´
                salary_match = re.search(r'(\d+)[kK]?[-~](\d+)[kK]?', salary_text)
                if salary_match:
                    job.salary_min = int(salary_match.group(1)) * 1000
                    job.salary_max = int(salary_match.group(2)) * 1000
            
            # å·¥ä½œåœ°ç‚¹
            location_elem = soup.find('input', {'name': 'positionAddress'})
            if location_elem:
                job.location = location_elem.get('value', '').strip()
            else:
                # å¤‡ç”¨æ–¹æ³•
                location_elem = soup.find('span', class_='add') or soup.find('em', class_='add')
                if location_elem:
                    job.location = location_elem.get_text(strip=True)
            
            # å·¥ä½œç»éªŒè¦æ±‚
            exp_elem = soup.find('dd', class_='job_request')
            if exp_elem:
                exp_text = exp_elem.get_text()
                exp_match = re.search(r'(\d+)[-~](\d+)å¹´', exp_text)
                if exp_match:
                    job.experience_min = int(exp_match.group(1))
                    job.experience_max = int(exp_match.group(2))
                elif 'ç»éªŒä¸é™' in exp_text:
                    job.experience_min = 0
                    job.experience_max = 0
            
            # å­¦å†è¦æ±‚
            edu_elem = soup.find('dd', class_='job_request')
            if edu_elem:
                edu_text = edu_elem.get_text()
                if 'æœ¬ç§‘' in edu_text:
                    job.education = 'æœ¬ç§‘'
                elif 'ç¡•å£«' in edu_text:
                    job.education = 'ç¡•å£«'
                elif 'å¤§ä¸“' in edu_text:
                    job.education = 'å¤§ä¸“'
                elif 'åšå£«' in edu_text:
                    job.education = 'åšå£«'
                elif 'å­¦å†ä¸é™' in edu_text:
                    job.education = 'ä¸é™'
            
            # èŒä½æè¿°
            desc_elem = soup.find('dd', class_='job_bt') or soup.find('div', class_='job_bt')
            if desc_elem:
                # ç§»é™¤HTMLæ ‡ç­¾ï¼Œä¿ç•™æ¢è¡Œ
                desc_text = desc_elem.get_text(separator='\n', strip=True)
                job.description = desc_text
            
            # æŠ€èƒ½è¦æ±‚ï¼ˆä»èŒä½æè¿°ä¸­æå–ï¼‰
            if job.description:
                job.skills = self._extract_skills_from_description(job.description)
            
            # å…¬å¸ä¿¡æ¯
            company_info_elem = soup.find('ul', class_='c_feature')
            if company_info_elem:
                company_features = []
                for li in company_info_elem.find_all('li'):
                    feature = li.get_text(strip=True)
                    if feature:
                        company_features.append(feature)
                job.company_info = ' | '.join(company_features)
            
            # å‘å¸ƒæ—¶é—´
            time_elem = soup.find('span', class_='time') or soup.find('time')
            if time_elem:
                time_text = time_elem.get_text(strip=True)
                job.published_time = time_text
            
            # è®¾ç½®æ¥æº
            job.source = 'lagou'
            job.crawled_at = datetime.now()
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not job.title or not job.company:
                self.logger.warning("èŒä½ä¿¡æ¯ä¸å®Œæ•´ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–")
                return None
            
            self.logger.info(f"æˆåŠŸæå–æ‹‰å‹¾ç½‘èŒä½: {job.title} - {job.company}")
            return job
            
        except Exception as e:
            self.logger.error(f"æå–èŒä½ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _extract_skills_from_description(self, description: str) -> List[str]:
        """ä»èŒä½æè¿°ä¸­æå–æŠ€èƒ½å…³é”®è¯"""
        # å¸¸è§æŠ€èƒ½å…³é”®è¯
        skill_patterns = [
            r'\b(Java|Python|JavaScript|TypeScript|C\+\+|C#|Go|PHP|Ruby|Scala|Kotlin)\b',
            r'\b(React|Vue|Angular|Spring|Django|Flask|Node\.js|Express)\b',
            r'\b(MySQL|PostgreSQL|MongoDB|Redis|Elasticsearch|Kafka)\b',
            r'\b(Docker|Kubernetes|Jenkins|Git|Linux|AWS|Azure)\b',
            r'\b(HTML|CSS|SASS|LESS|Webpack|Babel)\b',
            r'\b(æœºå™¨å­¦ä¹ |æ·±åº¦å­¦ä¹ |äººå·¥æ™ºèƒ½|æ•°æ®åˆ†æ|å¤§æ•°æ®)\b'
        ]
        
        skills = []
        description_upper = description.upper()
        
        for pattern in skill_patterns:
            matches = re.findall(pattern, description, re.IGNORECASE)
            skills.extend(matches)
        
        # å»é‡å¹¶æ¸…ç†
        skills = list(set(skill.strip() for skill in skills if skill.strip()))
        return skills[:10]  # é™åˆ¶æŠ€èƒ½æ•°é‡
    
    def scrape_job(self, url: str) -> ScrapingResult:
        """çˆ¬å–æ‹‰å‹¾ç½‘èŒä½ä¿¡æ¯"""
        start_time = time.time()
        
        try:
            # éªŒè¯URL
            if not self._validate_lagou_url(url):
                return ScrapingResult(
                    success=False,
                    error="ä¸æ˜¯æœ‰æ•ˆçš„æ‹‰å‹¾ç½‘èŒä½URL",
                    url=url,
                    scraped_at=datetime.now()
                )
            
            # æå–èŒä½ID
            job_id = self._extract_job_id(url)
            if not job_id:
                return ScrapingResult(
                    success=False,
                    error="æ— æ³•ä»URLä¸­æå–èŒä½ID",
                    url=url,
                    scraped_at=datetime.now()
                )
            
            # è®¾ç½®æµè§ˆå™¨
            self._setup_browser()
            
            self.logger.info(f"å¼€å§‹çˆ¬å–æ‹‰å‹¾ç½‘èŒä½: {url}")
            
            # è®¿é—®èŒä½é¡µé¢
            self.page.goto(url, wait_until='domcontentloaded', timeout=30000)
            
            # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
            self._simulate_human_behavior()
            
            # æå–èŒä½ä¿¡æ¯
            job = self._extract_job_info(job_id)
            
            if job:
                job.url = url
                response_time = time.time() - start_time
                
                return ScrapingResult(
                    success=True,
                    job=job,
                    url=url,
                    scraped_at=datetime.now()
                )
            else:
                return ScrapingResult(
                    success=False,
                    error="æå–èŒä½ä¿¡æ¯å¤±è´¥",
                    url=url,
                    scraped_at=datetime.now()
                )
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–å¤±è´¥: {e}")
            return ScrapingResult(
                success=False,
                error=str(e),
                url=url,
                scraped_at=datetime.now()
            )
        
        finally:
            self.cleanup()
    
    def test_connection(self) -> bool:
        """æµ‹è¯•æ‹‰å‹¾ç½‘è¿æ¥"""
        try:
            self._setup_browser()
            self.page.goto('https://www.lagou.com', timeout=30000)
            return True
        except Exception as e:
            self.logger.error(f"æ‹‰å‹¾ç½‘è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
        finally:
            self.cleanup()
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.page:
                self.page.close()
                self.page = None
            if self.context:
                self.context.close()
                self.context = None
            if self.browser:
                self.browser.close()
                self.browser = None
            if self.playwright:
                self.playwright.stop()
                self.playwright = None
        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")


# ä¾¿æ·å‡½æ•°
def scrape_lagou_job(url: str, headless: bool = False) -> ScrapingResult:
    """ä¾¿æ·å‡½æ•°ï¼šçˆ¬å–æ‹‰å‹¾ç½‘èŒä½"""
    scraper = LagouScraper(headless=headless)
    return scraper.scrape_job(url)


def test_lagou_connection(headless: bool = False) -> bool:
    """ä¾¿æ·å‡½æ•°ï¼šæµ‹è¯•æ‹‰å‹¾ç½‘è¿æ¥"""
    scraper = LagouScraper(headless=headless)
    return scraper.test_connection()