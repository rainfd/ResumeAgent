"""ç½‘é¡µçˆ¬è™«æ¨¡å—

æä¾›BOSSç›´è˜ç­‰æ‹›è˜ç½‘ç«™çš„èŒä½ä¿¡æ¯çˆ¬å–åŠŸèƒ½ã€‚
"""

import re
import time
import random
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse, urljoin
from dataclasses import dataclass
from datetime import datetime

try:
    import requests
    from bs4 import BeautifulSoup
    HAS_SCRAPING_SUPPORT = True
except ImportError:
    HAS_SCRAPING_SUPPORT = False

try:
    from playwright.sync_api import sync_playwright
    HAS_PLAYWRIGHT_SUPPORT = True
except ImportError:
    HAS_PLAYWRIGHT_SUPPORT = False

from ..utils import get_logger
from ..utils.errors import NetworkError, ResumeAssistantError
from .job_manager import Job

logger = get_logger(__name__)


@dataclass
class ScrapingResult:
    """çˆ¬å–ç»“æœæ•°æ®ç»“æ„"""
    success: bool
    job: Optional[Job] = None
    error: Optional[str] = None
    url: Optional[str] = None
    scraped_at: Optional[datetime] = None


class JobScraper:
    """èŒä½ä¿¡æ¯çˆ¬è™«åŸºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        if not HAS_SCRAPING_SUPPORT:
            raise ResumeAssistantError("çˆ¬è™«ä¾èµ–åº“æœªå®‰è£…ã€‚è¯·å®‰è£…: pip install requests beautifulsoup4")
        
        self.session = requests.Session()
        self._setup_session()
        self.logger = get_logger(self.__class__.__name__)
    
    def _setup_session(self):
        """è®¾ç½®HTTPä¼šè¯"""
        # è®¾ç½®ç”¨æˆ·ä»£ç†
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # è®¾ç½®è¶…æ—¶
        self.session.timeout = 30
    
    def scrape_job(self, url: str) -> ScrapingResult:
        """çˆ¬å–èŒä½ä¿¡æ¯
        
        Args:
            url: èŒä½é¡µé¢URL
            
        Returns:
            ScrapingResult: çˆ¬å–ç»“æœ
        """
        try:
            # éªŒè¯URL
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                return ScrapingResult(
                    success=False,
                    error="æ— æ•ˆçš„URLæ ¼å¼",
                    url=url
                )
            
            # æ ¹æ®åŸŸåé€‰æ‹©å¯¹åº”çš„çˆ¬è™«
            if 'zhipin.com' in parsed_url.netloc:
                return self._scrape_boss_job(url)
            else:
                return ScrapingResult(
                    success=False,
                    error=f"æš‚ä¸æ”¯æŒçš„ç½‘ç«™: {parsed_url.netloc}",
                    url=url
                )
        
        except Exception as e:
            self.logger.error(f"çˆ¬å–èŒä½ä¿¡æ¯å¤±è´¥ {url}: {e}")
            return ScrapingResult(
                success=False,
                error=str(e),
                url=url
            )
    
    def _scrape_boss_job(self, url: str) -> ScrapingResult:
        """çˆ¬å–BOSSç›´è˜èŒä½ä¿¡æ¯
        
        Args:
            url: BOSSç›´è˜èŒä½é¡µé¢URL
            
        Returns:
            ScrapingResult: çˆ¬å–ç»“æœ
        """
        try:
            # æ·»åŠ éšæœºå»¶æ—¶
            time.sleep(random.uniform(1, 3))
            
            # å‘é€HTTPè¯·æ±‚
            response = self._make_request(url)
            if not response:
                return ScrapingResult(
                    success=False,
                    error="è¯·æ±‚å¤±è´¥",
                    url=url
                )
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–èŒä½ä¿¡æ¯
            job_data = self._extract_boss_job_data(soup, url)
            if not job_data:
                return ScrapingResult(
                    success=False,
                    error="æœªèƒ½æå–åˆ°èŒä½ä¿¡æ¯",
                    url=url
                )
            
            # åˆ›å»ºJobå¯¹è±¡
            job = Job(
                id=job_data.get('id', ''),
                title=job_data.get('title', ''),
                company=job_data.get('company', ''),
                description=job_data.get('description', ''),
                requirements=job_data.get('requirements', ''),
                location=job_data.get('location', ''),
                salary=job_data.get('salary', ''),
                experience_level=job_data.get('experience_level', ''),
                education_level=job_data.get('education_level', ''),
                job_type=job_data.get('job_type', ''),
                tags=job_data.get('tags', []),
                company_info=job_data.get('company_info', {}),
                contact_info=job_data.get('contact_info', {}),
                source_url=url,
                created_at=datetime.now(),
                updated_at=datetime.now()
            )
            
            return ScrapingResult(
                success=True,
                job=job,
                url=url,
                scraped_at=datetime.now()
            )
        
        except Exception as e:
            self.logger.error(f"BOSSç›´è˜çˆ¬å–å¤±è´¥ {url}: {e}")
            return ScrapingResult(
                success=False,
                error=f"çˆ¬å–å¤±è´¥: {e}",
                url=url
            )
    
    def _make_request(self, url: str, max_retries: int = 3) -> Optional[requests.Response]:
        """å‘é€HTTPè¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            url: è¯·æ±‚URL
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            requests.Response: å“åº”å¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
        """
        for attempt in range(max_retries):
            try:
                self.logger.debug(f"è¯·æ±‚URL (å°è¯• {attempt + 1}/{max_retries}): {url}")
                
                response = self.session.get(url)
                response.raise_for_status()
                
                # æ£€æŸ¥æ˜¯å¦è¢«åçˆ¬
                if self._is_blocked_response(response):
                    self.logger.warning(f"ç–‘ä¼¼è¢«åçˆ¬æœºåˆ¶æ‹¦æˆª: {url}")
                    if attempt < max_retries - 1:
                        # å¢åŠ å»¶æ—¶åé‡è¯•
                        time.sleep(random.uniform(3, 8))
                        continue
                    else:
                        return None
                
                return response
            
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(2, 5))
                    continue
                else:
                    raise NetworkError(f"è¯·æ±‚å¤±è´¥: {e}", url=url)
        
        return None
    
    def _is_blocked_response(self, response: requests.Response) -> bool:
        """æ£€æŸ¥å“åº”æ˜¯å¦è¢«åçˆ¬æœºåˆ¶æ‹¦æˆª
        
        Args:
            response: HTTPå“åº”å¯¹è±¡
            
        Returns:
            bool: æ˜¯å¦è¢«æ‹¦æˆª
        """
        # æ£€æŸ¥å¸¸è§çš„åçˆ¬æ ‡è¯†
        blocked_indicators = [
            "éªŒè¯ç ",
            "captcha",
            "robot",
            "blocked",
            "è®¿é—®å—é™",
            "è¯·ç¨åå†è¯•"
        ]
        
        response_text = response.text.lower()
        for indicator in blocked_indicators:
            if indicator in response_text:
                return True
        
        # æ£€æŸ¥çŠ¶æ€ç 
        if response.status_code in [403, 429, 503]:
            return True
        
        # æ£€æŸ¥å“åº”å†…å®¹é•¿åº¦ï¼ˆè¿‡çŸ­å¯èƒ½æ˜¯è¢«æ‹¦æˆªï¼‰
        if len(response.text.strip()) < 1000:
            return True
        
        return False
    
    def _extract_boss_job_data(self, soup: BeautifulSoup, url: str) -> Optional[Dict[str, Any]]:
        """ä»BOSSç›´è˜é¡µé¢æå–èŒä½æ•°æ®
        
        Args:
            soup: BeautifulSoupè§£æå¯¹è±¡
            url: é¡µé¢URL
            
        Returns:
            Dict: èŒä½æ•°æ®å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            job_data = {}
            
            # ç”Ÿæˆå”¯ä¸€ID
            import hashlib
            job_data['id'] = hashlib.md5(url.encode()).hexdigest()[:8]
            
            # æå–èŒä½æ ‡é¢˜
            title_elem = soup.find('h1', class_='name') or soup.find('div', class_='job-title')
            if title_elem:
                job_data['title'] = title_elem.get_text(strip=True)
            
            # æå–å…¬å¸åç§°
            company_elem = soup.find('div', class_='company-name') or soup.find('a', class_='company-name')
            if company_elem:
                job_data['company'] = company_elem.get_text(strip=True)
            
            # æå–è–ªèµ„èŒƒå›´
            salary_elem = soup.find('span', class_='salary') or soup.find('div', class_='job-salary')
            if salary_elem:
                job_data['salary'] = salary_elem.get_text(strip=True)
            
            # æå–å·¥ä½œåœ°ç‚¹
            location_elem = soup.find('p', class_='job-location') or soup.find('span', class_='job-area')
            if location_elem:
                job_data['location'] = location_elem.get_text(strip=True)
            
            # æå–ç»éªŒè¦æ±‚
            exp_elem = soup.find('p', class_='job-experience') or soup.find('span', class_='job-experience')
            if exp_elem:
                job_data['experience_level'] = exp_elem.get_text(strip=True)
            
            # æå–å­¦å†è¦æ±‚
            edu_elem = soup.find('p', class_='job-degree') or soup.find('span', class_='job-degree')
            if edu_elem:
                job_data['education_level'] = edu_elem.get_text(strip=True)
            
            # æå–èŒä½æè¿°
            desc_elem = soup.find('div', class_='job-sec') or soup.find('div', class_='job-detail')
            if desc_elem:
                job_data['description'] = desc_elem.get_text(strip=True)
                job_data['requirements'] = job_data['description']  # æš‚æ—¶è®¾ä¸ºç›¸åŒ
            
            # æå–èŒä½æ ‡ç­¾
            tags = []
            tag_elems = soup.find_all('span', class_='job-tag') or soup.find_all('div', class_='job-tags')
            for tag_elem in tag_elems:
                tag_text = tag_elem.get_text(strip=True)
                if tag_text:
                    tags.append(tag_text)
            job_data['tags'] = tags
            
            # æå–å…¬å¸ä¿¡æ¯
            company_info = {}
            company_size_elem = soup.find('p', class_='company-size')
            if company_size_elem:
                company_info['size'] = company_size_elem.get_text(strip=True)
            
            company_type_elem = soup.find('p', class_='company-type')
            if company_type_elem:
                company_info['type'] = company_type_elem.get_text(strip=True)
            
            job_data['company_info'] = company_info
            
            # æå–è”ç³»ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            job_data['contact_info'] = {}
            
            # è®¾ç½®èŒä½ç±»å‹
            job_data['job_type'] = 'å…¨èŒ'  # é»˜è®¤å€¼
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not job_data.get('title') or not job_data.get('company'):
                self.logger.warning("èŒä½æ ‡é¢˜æˆ–å…¬å¸åç§°ä¸ºç©º")
                return None
            
            return job_data
        
        except Exception as e:
            self.logger.error(f"æå–èŒä½æ•°æ®å¤±è´¥: {e}")
            return None
    
    def test_connection(self, url: str) -> bool:
        """æµ‹è¯•ç½‘ç»œè¿æ¥
        
        Args:
            url: æµ‹è¯•URL
            
        Returns:
            bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            response = self.session.head(url, timeout=10)
            return response.status_code == 200
        except Exception as e:
            self.logger.error(f"è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False


class BossZhipinScraper(JobScraper):
    """BOSSç›´è˜ä¸“ç”¨çˆ¬è™«"""
    
    def __init__(self):
        super().__init__()
        
        # BOSSç›´è˜ç‰¹å®šè®¾ç½®
        self.session.headers.update({
            'Referer': 'https://www.zhipin.com/',
            'Host': 'www.zhipin.com'
        })
    
    def scrape_search_results(self, search_url: str, max_pages: int = 3) -> List[ScrapingResult]:
        """çˆ¬å–æœç´¢ç»“æœé¡µé¢çš„èŒä½åˆ—è¡¨
        
        Args:
            search_url: æœç´¢é¡µé¢URL
            max_pages: æœ€å¤§çˆ¬å–é¡µæ•°
            
        Returns:
            List[ScrapingResult]: çˆ¬å–ç»“æœåˆ—è¡¨
        """
        results = []
        
        try:
            for page in range(1, max_pages + 1):
                self.logger.info(f"çˆ¬å–ç¬¬ {page} é¡µ: {search_url}")
                
                # æ„é€ åˆ†é¡µURL
                page_url = f"{search_url}&page={page}" if '?' in search_url else f"{search_url}?page={page}"
                
                # è·å–é¡µé¢å†…å®¹
                response = self._make_request(page_url)
                if not response:
                    continue
                
                # è§£æé¡µé¢ï¼Œæå–èŒä½é“¾æ¥
                soup = BeautifulSoup(response.text, 'html.parser')
                job_links = self._extract_job_links(soup)
                
                if not job_links:
                    self.logger.warning(f"ç¬¬ {page} é¡µæœªæ‰¾åˆ°èŒä½é“¾æ¥")
                    break
                
                # çˆ¬å–æ¯ä¸ªèŒä½è¯¦æƒ…
                for job_link in job_links:
                    full_url = urljoin(search_url, job_link)
                    result = self.scrape_job(full_url)
                    results.append(result)
                    
                    # æ·»åŠ å»¶æ—¶é¿å…è¢«å°
                    time.sleep(random.uniform(2, 4))
                
                # é¡µé¢é—´å»¶æ—¶
                time.sleep(random.uniform(3, 6))
        
        except Exception as e:
            self.logger.error(f"æœç´¢ç»“æœçˆ¬å–å¤±è´¥: {e}")
        
        return results
    
    def _extract_job_links(self, soup: BeautifulSoup) -> List[str]:
        """ä»æœç´¢ç»“æœé¡µé¢æå–èŒä½é“¾æ¥
        
        Args:
            soup: BeautifulSoupè§£æå¯¹è±¡
            
        Returns:
            List[str]: èŒä½é“¾æ¥åˆ—è¡¨
        """
        links = []
        
        try:
            # BOSSç›´è˜èŒä½é“¾æ¥é€‰æ‹©å™¨
            job_elements = soup.find_all('a', href=re.compile(r'/job_detail/'))
            
            for elem in job_elements:
                href = elem.get('href')
                if href and href.startswith('/job_detail/'):
                    links.append(href)
            
            # å»é‡
            links = list(set(links))
            
        except Exception as e:
            self.logger.error(f"æå–èŒä½é“¾æ¥å¤±è´¥: {e}")
        
        return links


class PlaywrightScraper(JobScraper):
    """åŸºäºPlaywrightçš„èŒä½çˆ¬è™«"""
    
    def __init__(self, headless: bool = False, user_data_dir: Optional[str] = None):
        """åˆå§‹åŒ–Playwrightçˆ¬è™«
        
        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼ŒFalseä¸ºæœ‰å¤´æ¨¡å¼ï¼ˆæ›´éš¾è¢«æ£€æµ‹ï¼‰
            user_data_dir: ç”¨æˆ·æ•°æ®ç›®å½•è·¯å¾„ï¼Œç”¨äºä¿æŒç™»å½•çŠ¶æ€
        """
        if not HAS_PLAYWRIGHT_SUPPORT:
            raise ResumeAssistantError("Playwrightä¾èµ–åº“æœªå®‰è£…ã€‚è¯·å®‰è£…: pip install playwright")
        
        self.logger = get_logger(self.__class__.__name__)
        self.playwright = None
        self.browser = None
        self.context = None
        self.headless = headless
        self.user_data_dir = user_data_dir
    
    def _setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨"""
        if not self.playwright:
            self.playwright = sync_playwright().start()
        
        # æµè§ˆå™¨å¯åŠ¨å‚æ•°ï¼ˆå¢å¼ºåæ£€æµ‹ï¼‰
        launch_args = [
            '--no-sandbox',
            '--disable-blink-features=AutomationControlled',
            '--disable-web-security',
            '--disable-features=VizDisplayCompositor',
            '--disable-dev-shm-usage',
            '--disable-extensions-except=',
            '--disable-plugins-discovery',
            '--disable-javascript-harmony-shipping',
            '--no-first-run',
            '--no-default-browser-check',
            '--disable-default-apps',
            '--disable-popup-blocking',
            '--disable-translate',
            '--disable-background-timer-throttling',
            '--disable-renderer-backgrounding',
            '--disable-device-discovery-notifications',
            '--disable-backgrounding-occluded-windows',
            '--disable-ipc-flooding-protection'
        ]
        
        # å¦‚æœä¸æ˜¯æ— å¤´æ¨¡å¼ï¼Œç§»é™¤ä¸€äº›å¯èƒ½å½±å“æ˜¾ç¤ºçš„å‚æ•°
        if not self.headless:
            # ç§»é™¤å¯èƒ½å½±å“æ˜¾ç¤ºçš„å‚æ•°
            launch_args = [arg for arg in launch_args if 'disable-images' not in arg]
            self.logger.info("å¯åŠ¨æœ‰å¤´æµè§ˆå™¨æ¨¡å¼ï¼ˆæ›´éš¾è¢«æ£€æµ‹ï¼‰")
        else:
            # æ— å¤´æ¨¡å¼æ·»åŠ æ›´å¤šä¼˜åŒ–å‚æ•°
            launch_args.extend([
                '--disable-images',  # ç¦ç”¨å›¾ç‰‡åŠ è½½ä»¥æé«˜é€Ÿåº¦
                '--disable-audio-output'
            ])
            
        # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡é€‰é¡¹
        context_options = {
            'user_agent': self._get_random_user_agent(),
            'viewport': {'width': 1920, 'height': 1080},
            'extra_http_headers': {
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none'
            }
        }
        
        # å¦‚æœæŒ‡å®šäº†ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡
        if self.user_data_dir:
            self.logger.info(f"ä½¿ç”¨ç”¨æˆ·æ•°æ®ç›®å½•: {self.user_data_dir}")
            # ä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡
            self.context = self.playwright.chromium.launch_persistent_context(
                user_data_dir=self.user_data_dir,
                headless=self.headless,
                args=launch_args,
                **context_options
            )
            # æŒä¹…åŒ–ä¸Šä¸‹æ–‡æœ¬èº«å°±æ˜¯browserå’Œcontextçš„ç»„åˆ
            self.browser = None  # æŒä¹…åŒ–æ¨¡å¼ä¸‹ä¸éœ€è¦å•ç‹¬çš„browserå¯¹è±¡
        else:
            # å¸¸è§„æ¨¡å¼ï¼šå…ˆåˆ›å»ºbrowserï¼Œå†åˆ›å»ºcontext
            self.browser = self.playwright.chromium.launch(
                headless=self.headless,
                args=launch_args
            )
            self.context = self.browser.new_context(**context_options)
    
    def _cleanup_browser(self):
        """æ¸…ç†æµè§ˆå™¨èµ„æº"""
        try:
            if self.context:
                self.context.close()
            if self.browser:
                self.browser.close()
            if self.playwright:
                self.playwright.stop()
        except Exception as e:
            self.logger.warning(f"æ¸…ç†æµè§ˆå™¨èµ„æºå¤±è´¥: {e}")
        finally:
            self.context = None
            self.browser = None
            self.playwright = None
    
    def _get_random_user_agent(self) -> str:
        """è·å–éšæœºUser-Agent"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        return random.choice(user_agents)
    
    def _wait_for_manual_verification(self, page, timeout: int = 120) -> bool:
        """ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å¤„ç†éªŒè¯ç æˆ–ç™»å½•
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            timeout: ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸé€šè¿‡éªŒè¯
        """
        try:
            if self.headless:
                self.logger.warning("æ£€æµ‹åˆ°éªŒè¯ç ä½†å½“å‰ä¸ºæ— å¤´æ¨¡å¼ï¼Œæ— æ³•æ‰‹åŠ¨å¤„ç†")
                return False
            
            # æ˜¾çœ¼çš„æç¤ºä¿¡æ¯
            print("\n" + "="*80)
            print("ğŸ¤– æ£€æµ‹åˆ°éœ€è¦äººå·¥éªŒè¯ï¼")
            print("="*80)
            print("ğŸ“‹ è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š")
            print("   1. åœ¨æ‰“å¼€çš„æµè§ˆå™¨çª—å£ä¸­å®ŒæˆIPéªŒè¯æˆ–äººæœºéªŒè¯")
            print("   2. å¦‚æœå‡ºç°æ»‘å—éªŒè¯ï¼Œè¯·æ‹–åŠ¨æ»‘å—åˆ°æ­£ç¡®ä½ç½®")
            print("   3. å¦‚æœå‡ºç°éªŒè¯ç ï¼Œè¯·æŒ‰æç¤ºè¾“å…¥éªŒè¯ç ")
            print("   4. ç­‰å¾…é¡µé¢è·³è½¬åˆ°æ­£å¸¸çš„èŒä½è¯¦æƒ…é¡µé¢")
            print("   5. ä¸è¦å…³é—­æµè§ˆå™¨çª—å£ï¼Œç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å®ŒæˆçŠ¶æ€")
            print()
            print(f"â° æœ€å¤§ç­‰å¾…æ—¶é—´: {timeout} ç§’")
            print(f"ğŸŒ å½“å‰é¡µé¢: {page.url}")
            print("ğŸ’¡ æç¤º: å¦‚éœ€å–æ¶ˆæ“ä½œï¼Œè¯·å…³é—­æµè§ˆå™¨çª—å£")
            print("="*80)
            
            self.logger.info("ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å®ŒæˆéªŒè¯...")
            
            # ç­‰å¾…é¡µé¢URLå˜åŒ–æˆ–ç‰¹å®šå…ƒç´ æ¶ˆå¤±ï¼Œè¡¨ç¤ºç”¨æˆ·å·²å¤„ç†å®ŒéªŒè¯
            start_time = time.time()
            current_url = page.url
            check_interval = 2  # ç¼©çŸ­æ£€æŸ¥é—´éš”åˆ°2ç§’ï¼Œæ›´å¿«å“åº”
            
            # å…ˆç­‰å¾…2ç§’è®©ç”¨æˆ·çœ‹åˆ°æç¤º
            time.sleep(2)
            
            while time.time() - start_time < timeout:
                try:
                    elapsed = int(time.time() - start_time)
                    remaining = timeout - elapsed
                    
                    # æ›´é¢‘ç¹æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10ç§’ï¼‰
                    if elapsed % 10 == 0 and elapsed > 0:
                        print(f"â³ ç­‰å¾…ä¸­... ({elapsed}/{timeout}ç§’) å‰©ä½™: {remaining}ç§’")
                    
                    # é¦–å…ˆå¿«é€Ÿæ£€æŸ¥æ˜¯å¦ä¸å†æ˜¯éªŒè¯é¡µé¢
                    is_still_blocked = self._check_blocked_page(page)
                    
                    if not is_still_blocked:
                        # éªŒè¯é¡µé¢ç‰¹å¾æ¶ˆå¤±ï¼Œç¡®è®¤éªŒè¯é€šè¿‡
                        print("âœ… éªŒè¯å®Œæˆï¼é¡µé¢æ¢å¤æ­£å¸¸")
                        self.logger.info("éªŒè¯é¡µé¢ç‰¹å¾æ¶ˆå¤±ï¼ŒéªŒè¯å®Œæˆ")
                        return True
                    
                    # æ£€æŸ¥URLæ˜¯å¦å˜åŒ–ï¼ˆå¯èƒ½è¡¨ç¤ºè·³è½¬ï¼‰
                    new_url = page.url
                    if new_url != current_url:
                        print(f"ğŸ”„ æ£€æµ‹åˆ°é¡µé¢è·³è½¬: {new_url}")
                        current_url = new_url
                        
                        # ç­‰å¾…æ–°é¡µé¢ç¨³å®š
                        time.sleep(2)
                        
                        # å†æ¬¡æ£€æŸ¥æ–°é¡µé¢æ˜¯å¦éœ€è¦éªŒè¯
                        if not self._check_blocked_page(page):
                            print("âœ… è·³è½¬åçš„é¡µé¢éªŒè¯é€šè¿‡ï¼")
                            return True
                        else:
                            print("âš ï¸ æ–°é¡µé¢ä»éœ€éªŒè¯ï¼Œç»§ç»­ç­‰å¾…...")
                    
                    # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦å·²å…³é—­ï¼ˆç”¨æˆ·å–æ¶ˆæ“ä½œï¼‰
                    try:
                        page.title()  # å°è¯•è®¿é—®é¡µé¢ï¼Œå¦‚æœæµè§ˆå™¨å…³é—­ä¼šæŠ›å‡ºå¼‚å¸¸
                    except Exception:
                        print("ğŸš« æ£€æµ‹åˆ°æµè§ˆå™¨å·²å…³é—­ï¼Œå–æ¶ˆéªŒè¯")
                        return False
                    
                    time.sleep(check_interval)
                    
                except Exception as e:
                    self.logger.debug(f"éªŒè¯çŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
                    # å¯èƒ½æ˜¯é¡µé¢æ­£åœ¨åŠ è½½ï¼Œç»§ç»­ç­‰å¾…
                    time.sleep(check_interval)
            
            print("\nâŒ æ‰‹åŠ¨éªŒè¯è¶…æ—¶")
            print("ğŸ’¡ æç¤º: å¦‚æœæ‚¨å·²å®ŒæˆéªŒè¯ä½†ç¨‹åºä»ç„¶è¶…æ—¶ï¼Œå¯ä»¥:")
            print("   - å°è¯•åˆ·æ–°é¡µé¢")
            print("   - é‡æ–°è¿è¡Œç¨‹åº")
            return False
            
        except Exception as e:
            self.logger.error(f"æ‰‹åŠ¨éªŒè¯å¤„ç†å¤±è´¥: {e}")
            return False
    
    def scrape_job(self, url: str) -> ScrapingResult:
        """ä½¿ç”¨Playwrightçˆ¬å–èŒä½ä¿¡æ¯
        
        Args:
            url: èŒä½é¡µé¢URL
            
        Returns:
            ScrapingResult: çˆ¬å–ç»“æœ
        """
        try:
            # éªŒè¯URL
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                return ScrapingResult(
                    success=False,
                    error="æ— æ•ˆçš„URLæ ¼å¼",
                    url=url
                )
            
            # è®¾ç½®æµè§ˆå™¨
            self._setup_browser()
            
            # åˆ›å»ºé¡µé¢
            page = self.context.new_page()
            
            try:
                self.logger.info(f"ä½¿ç”¨Playwrightè®¿é—®: {url}")
                
                # æ³¨å…¥å¢å¼ºåæ£€æµ‹è„šæœ¬
                page.add_init_script("""
                    // éšè—webdriverå±æ€§
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined,
                    });
                    
                    // ä¼ªé€ chromeå¯¹è±¡
                    window.chrome = {
                        runtime: {},
                        app: {
                            isInstalled: false,
                        },
                        webstore: {
                            onInstallStageChanged: {},
                            onDownloadProgress: {},
                        },
                    };
                    
                    // ä¼ªé€ æ’ä»¶æ•°ç»„
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => {
                            return [
                                {
                                    0: {type: "application/x-google-chrome-pdf", suffixes: "pdf", description: "Portable Document Format", enabledPlugin: Plugin},
                                    description: "Portable Document Format",
                                    filename: "internal-pdf-viewer",
                                    length: 1,
                                    name: "Chrome PDF Plugin"
                                },
                                {
                                    0: {type: "application/pdf", suffixes: "pdf", description: "", enabledPlugin: Plugin},
                                    description: "",
                                    filename: "mhjfbmdgcfjbbpaeojofohoefgiehjai",
                                    length: 1,
                                    name: "Chrome PDF Viewer"
                                }
                            ];
                        },
                    });
                    
                    // ä¼ªé€ æƒé™æŸ¥è¯¢
                    const originalQuery = window.navigator.permissions.query;
                    window.navigator.permissions.query = (parameters) => (
                        parameters.name === 'notifications' ?
                            Promise.resolve({ state: Notification.permission }) :
                            originalQuery(parameters)
                    );
                    
                    // ä¼ªé€ è¯­è¨€å’Œå¹³å°
                    Object.defineProperty(navigator, 'languages', {
                        get: () => ['zh-CN', 'zh', 'en-US', 'en'],
                    });
                    
                    Object.defineProperty(navigator, 'platform', {
                        get: () => 'Win32',
                    });
                    
                    // ç§»é™¤è‡ªåŠ¨åŒ–æ£€æµ‹ç‰¹å¾
                    delete window.cdc_adoQpoasnfa76pfcZLmcfl_Array;
                    delete window.cdc_adoQpoasnfa76pfcZLmcfl_Promise;
                    delete window.cdc_adoQpoasnfa76pfcZLmcfl_Symbol;
                """)
                
                # è®¿é—®é¡µé¢ï¼Œç­‰å¾…é¡µé¢åŠ è½½
                page.goto(url, wait_until='domcontentloaded', timeout=30000)
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©JavaScriptæ‰§è¡Œ
                page.wait_for_timeout(random.uniform(3000, 6000))
                
                # æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º
                page.evaluate("window.scrollTo(0, document.body.scrollHeight / 3)")
                page.wait_for_timeout(random.uniform(1000, 2000))
                page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
                page.wait_for_timeout(random.uniform(1000, 2000))
                
                # é¦–æ¬¡æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†éªŒè¯ç æˆ–ç™»å½•
                verification_detected = self._check_blocked_page(page)
                
                if verification_detected:
                    page_title = page.title()
                    page_url = page.url
                    self.logger.warning(f"æ£€æµ‹åˆ°éªŒè¯é¡µé¢: {page_title} - {page_url}")
                    
                    # å¦‚æœæ˜¯æœ‰å¤´æ¨¡å¼ï¼Œå°è¯•ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å¤„ç†
                    if not self.headless:
                        print(f"\nğŸ” å½“å‰é¡µé¢æ ‡é¢˜: {page_title}")
                        print(f"ğŸ”— é¡µé¢URL: {page_url}")
                        
                        # è¿›å…¥æ‰‹åŠ¨éªŒè¯æµç¨‹
                        if self._wait_for_manual_verification(page):
                            print("âœ… æ‰‹åŠ¨éªŒè¯å®Œæˆï¼Œç»§ç»­çˆ¬å–")
                            self.logger.info("æ‰‹åŠ¨éªŒè¯å®Œæˆï¼Œç»§ç»­çˆ¬å–")
                            
                            # éªŒè¯å®Œæˆåå†æ¬¡ç­‰å¾…é¡µé¢ç¨³å®š
                            page.wait_for_timeout(3000)
                        else:
                            print("âŒ éªŒè¯å¤„ç†å¤±è´¥æˆ–è¶…æ—¶")
                            return ScrapingResult(
                                success=False,
                                error="éªŒè¯ç å¤„ç†å¤±è´¥æˆ–è¶…æ—¶",
                                url=url
                            )
                    else:
                        return ScrapingResult(
                            success=False,
                            error="é¡µé¢è¢«æ‹¦æˆªéœ€è¦éªŒè¯ï¼Œå»ºè®®ä½¿ç”¨æœ‰å¤´æ¨¡å¼",
                            url=url
                        )
                else:
                    # æ²¡æœ‰æ£€æµ‹åˆ°éªŒè¯ï¼Œè®°å½•æ­£å¸¸è®¿é—®
                    self.logger.info(f"é¡µé¢æ­£å¸¸è®¿é—®ï¼Œæ— éœ€éªŒè¯: {page.title()}")
                
                # è·å–é¡µé¢HTMLå†…å®¹
                html_content = page.content()
                
                # ä½¿ç”¨BeautifulSoupè§£æ
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # æ ¹æ®ç½‘ç«™ç±»å‹æå–ä¿¡æ¯
                if 'zhipin.com' in parsed_url.netloc:
                    job_data = self._extract_boss_job_data_playwright(soup, page, url)
                else:
                    job_data = self._extract_generic_job_data(soup, url)
                
                if not job_data:
                    return ScrapingResult(
                        success=False,
                        error="æœªèƒ½æå–åˆ°èŒä½ä¿¡æ¯",
                        url=url
                    )
                
                # åˆ›å»ºJobå¯¹è±¡
                job = Job(
                    id=job_data.get('id', ''),
                    title=job_data.get('title', ''),
                    company=job_data.get('company', ''),
                    description=job_data.get('description', ''),
                    requirements=job_data.get('requirements', ''),
                    location=job_data.get('location', ''),
                    salary=job_data.get('salary', ''),
                    experience_level=job_data.get('experience_level', ''),
                    education_level=job_data.get('education_level', ''),
                    job_type=job_data.get('job_type', ''),
                    tags=job_data.get('tags', []),
                    company_info=job_data.get('company_info', {}),
                    contact_info=job_data.get('contact_info', {}),
                    source_url=url,
                    created_at=datetime.now(),
                    updated_at=datetime.now()
                )
                
                return ScrapingResult(
                    success=True,
                    job=job,
                    url=url,
                    scraped_at=datetime.now()
                )
                
            finally:
                page.close()
                
        except Exception as e:
            self.logger.error(f"Playwrightçˆ¬å–å¤±è´¥ {url}: {e}")
            return ScrapingResult(
                success=False,
                error=f"çˆ¬å–å¤±è´¥: {e}",
                url=url
            )
        finally:
            self._cleanup_browser()
    
    def _check_blocked_page(self, page) -> bool:
        """æ£€æŸ¥é¡µé¢æ˜¯å¦è¢«æ‹¦æˆª"""
        try:
            # è·å–é¡µé¢åŸºæœ¬ä¿¡æ¯
            title = page.title().lower()
            url = page.url.lower()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ˜ç¡®çš„éªŒè¯é¡µé¢æ ‡é¢˜
            verification_titles = [
                'å®‰å…¨éªŒè¯',
                'äººæœºéªŒè¯', 
                'æ»‘å—éªŒè¯',
                'éªŒè¯ç éªŒè¯',
                'security verification',
                'captcha verification',
                'è¯·å®ŒæˆéªŒè¯'
            ]
            
            # æ’é™¤èŒä½ç›¸å…³çš„æ ‡é¢˜
            job_indicators = ['å·¥ç¨‹å¸ˆ', 'ä¸“å®¶', 'å²—ä½', 'èŒä½', 'æ‹›è˜', '-', 'å…¬å¸']
            is_job_page = any(indicator in title for indicator in job_indicators)
            
            if not is_job_page:  # åªæœ‰åœ¨ä¸æ˜¯èŒä½é¡µé¢æ—¶æ‰æ£€æŸ¥éªŒè¯æ ‡é¢˜
                for verification_title in verification_titles:
                    if verification_title in title:
                        self.logger.info(f"æ£€æµ‹åˆ°éªŒè¯é¡µé¢æ ‡é¢˜: {title}")
                        return True
            
            # æ£€æŸ¥URLæ˜¯å¦åŒ…å«éªŒè¯ç›¸å…³è·¯å¾„
            verification_urls = [
                '/captcha/',
                '/verification/',
                '/security/',
                '/verify',
                '/challenge'
            ]
            
            for verification_url in verification_urls:
                if verification_url in url:
                    self.logger.info(f"æ£€æµ‹åˆ°éªŒè¯é¡µé¢URL: {url}")
                    return True
            
            # æ£€æŸ¥é¡µé¢å†…å®¹ä¸­çš„éªŒè¯å…ƒç´ 
            try:
                # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯èŒä½é¡µé¢ - å¦‚æœæ˜¯èŒä½é¡µé¢ï¼Œåˆ™ä¸å¤ªå¯èƒ½æ˜¯éªŒè¯é¡µé¢
                job_page_indicators = [
                    '.job-detail',
                    '.job-name',  
                    '.job-title',
                    '.position-detail',
                    '.company-info',
                    '.salary-info',
                    '[class*="job-"]',
                    '.boss-info'
                ]
                
                is_job_page = False
                for indicator in job_page_indicators:
                    if page.query_selector(indicator):
                        is_job_page = True
                        break
                
                # å¦‚æœç¡®å®šæ˜¯èŒä½é¡µé¢ï¼Œåˆ™ä¸æ£€æŸ¥éªŒè¯å…ƒç´ ï¼ˆé¿å…è¯¯åˆ¤ï¼‰
                if is_job_page:
                    self.logger.debug("æ£€æµ‹åˆ°èŒä½é¡µé¢ï¼Œè·³è¿‡éªŒè¯å…ƒç´ æ£€æŸ¥")
                    return False
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ç›¸å…³çš„ç‰¹å®šå…ƒç´ ï¼ˆæ›´ä¸¥æ ¼çš„é€‰æ‹©å™¨ï¼‰
                verification_selectors = [
                    '.captcha-container',
                    '.verification-container', 
                    '.slide-verify',
                    '.geetest_captcha',  # æ›´å…·ä½“çš„æéªŒéªŒè¯
                    '.yidun_panel',  # ç½‘æ˜“äº‘ç›¾éªŒè¯
                    '.nc_wrapper',  # é˜¿é‡Œäº‘æ»‘å—éªŒè¯
                    'div[class*="verify"]',  # éªŒè¯ç›¸å…³div
                    'form[class*="verify"]'  # éªŒè¯ç›¸å…³è¡¨å•
                ]
                
                for selector in verification_selectors: 
                    element = page.query_selector(selector)
                    if element and element.is_visible():  # ç¡®ä¿å…ƒç´ å¯è§
                        self.logger.info(f"æ£€æµ‹åˆ°éªŒè¯å…ƒç´ : {selector}")
                        return True
                
                # æ£€æŸ¥ç‰¹å®šçš„éªŒè¯æ–‡æœ¬ï¼ˆæ›´ç²¾ç¡®ï¼‰
                body_text = page.inner_text('body') if page.query_selector('body') else ''
                
                # åªæ£€æŸ¥æ˜ç¡®çš„éªŒè¯æç¤ºæ–‡æœ¬
                specific_verification_texts = [
                    'è¯·æ‹–åŠ¨æ»‘å—å®ŒæˆéªŒè¯',
                    'ç‚¹å‡»æŒ‰é’®è¿›è¡ŒéªŒè¯',
                    'è¯·ç‚¹å‡»å®ŒæˆéªŒè¯',
                    'å®‰å…¨éªŒè¯ä¸­',
                    'äººæœºèº«ä»½éªŒè¯',
                    'è¯·è¾“å…¥éªŒè¯ç ',
                    'éªŒè¯å¤±è´¥ï¼Œè¯·é‡è¯•',
                    'ä¸ºäº†ä¿æŠ¤è´¦å·å®‰å…¨'
                ]
                
                for verification_text in specific_verification_texts:
                    if verification_text in body_text:
                        self.logger.info(f"æ£€æµ‹åˆ°éªŒè¯æ–‡æœ¬: {verification_text}")
                        return True
                
                # æ£€æŸ¥é¡µé¢å†…å®¹é•¿åº¦ï¼ŒéªŒè¯é¡µé¢é€šå¸¸å†…å®¹è¾ƒå°‘ä¸”åŒ…å«ç‰¹å®šéªŒè¯è¯æ±‡
                if len(body_text.strip()) < 200:
                    verification_indicators = ['è¯·å®ŒæˆéªŒè¯', 'äººæœºéªŒè¯', 'å®‰å…¨éªŒè¯', 'æ»‘å—éªŒè¯', 'éªŒè¯ç ']
                    if any(indicator in body_text for indicator in verification_indicators):
                        self.logger.info(f"æ£€æµ‹åˆ°ç®€çŸ­çš„éªŒè¯é¡µé¢å†…å®¹: {len(body_text)} å­—ç¬¦")
                        return True
                    
            except Exception as e:
                self.logger.debug(f"éªŒè¯å…ƒç´ æ£€æŸ¥å¤±è´¥: {e}")
            
            return False
            
        except Exception as e:
            self.logger.warning(f"æ£€æŸ¥é¡µé¢æ‹¦æˆªçŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def _extract_boss_job_data_playwright(self, soup: BeautifulSoup, page, url: str) -> Optional[Dict[str, Any]]:
        """ä»BOSSç›´è˜é¡µé¢æå–èŒä½æ•°æ®ï¼ˆPlaywrightç‰ˆæœ¬ï¼‰"""
        try:
            job_data = {}
            
            # ç”Ÿæˆå”¯ä¸€ID
            import hashlib
            job_data['id'] = hashlib.md5(url.encode()).hexdigest()[:8]
            
            # å°è¯•ä½¿ç”¨æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
            selectors = {
                'title': [
                    '.job-title',
                    '.job-name', 
                    'h1.name',
                    '[class*="job"][class*="title"]',
                    'h1',
                    '.position-head h1'
                ],
                'company': [
                    '.company-name a',
                    '.company-name',
                    '[class*="company"][class*="name"]',
                    '.info-company h3',
                    '.company-info .name'
                ],
                'salary': [
                    '.salary',
                    '.job-salary',
                    '[class*="salary"]',
                    '.position-salary'
                ],
                'location': [
                    '.job-area',
                    '.job-location', 
                    '[class*="location"]',
                    '.position-location'
                ],
                'experience': [
                    '.job-experience',
                    '[class*="experience"]',
                    '.position-require'
                ],
                'education': [
                    '.job-degree',
                    '[class*="degree"]',
                    '[class*="education"]'
                ]
            }
            
            # æå–åŸºæœ¬ä¿¡æ¯
            for field, selector_list in selectors.items():
                for selector in selector_list:
                    try:
                        element = soup.select_one(selector)
                        if element:
                            text = element.get_text(strip=True)
                            if text:
                                if field == 'title':
                                    job_data['title'] = text
                                elif field == 'company': 
                                    job_data['company'] = text
                                elif field == 'salary':
                                    job_data['salary'] = text
                                elif field == 'location':
                                    job_data['location'] = text
                                elif field == 'experience':
                                    job_data['experience_level'] = text
                                elif field == 'education':
                                    job_data['education_level'] = text
                                break
                    except Exception:
                        continue
            
            # æå–èŒä½æè¿°
            desc_selectors = [
                '.job-sec',
                '.job-detail', 
                '.job-description',
                '[class*="job"][class*="desc"]',
                '.position-detail'
            ]
            
            for selector in desc_selectors:
                try:
                    element = soup.select_one(selector)
                    if element:
                        description = element.get_text(strip=True)
                        if description and len(description) > 50:  # ç¡®ä¿ä¸æ˜¯ç©ºå†…å®¹
                            job_data['description'] = description
                            job_data['requirements'] = description  # æš‚æ—¶è®¾ä¸ºç›¸åŒ
                            break
                except Exception:
                    continue
            
            # æå–èŒä½æ ‡ç­¾
            tags = []
            tag_selectors = ['.job-tag', '.position-tag', '[class*="tag"]']
            
            for selector in tag_selectors:
                try:
                    elements = soup.select(selector)
                    for elem in elements:
                        tag_text = elem.get_text(strip=True)
                        if tag_text and tag_text not in tags:
                            tags.append(tag_text)
                except Exception:
                    continue
            
            job_data['tags'] = tags
            job_data['company_info'] = {}
            job_data['contact_info'] = {}
            job_data['job_type'] = 'å…¨èŒ'
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not job_data.get('title'):
                # å°è¯•ä»é¡µé¢æ ‡é¢˜æå–
                page_title = soup.find('title')
                if page_title:
                    title_text = page_title.get_text()
                    # ä»æ ‡é¢˜ä¸­æå–èŒä½åç§°ï¼ˆé€šå¸¸æ ¼å¼ä¸º"èŒä½å-å…¬å¸å-BOSSç›´è˜"ï¼‰
                    if '-' in title_text:
                        job_data['title'] = title_text.split('-')[0].strip()
            
            if not job_data.get('company'):
                # å°è¯•ä»URLæˆ–å…¶ä»–åœ°æ–¹æå–å…¬å¸ä¿¡æ¯
                job_data['company'] = 'æœªçŸ¥å…¬å¸'
            
            # ç¡®ä¿æœ‰åŸºæœ¬ä¿¡æ¯
            if not job_data.get('title') or not job_data.get('company'):
                self.logger.warning("å…³é”®èŒä½ä¿¡æ¯ç¼ºå¤±")
                return None
            
            # è®¾ç½®é»˜è®¤å€¼
            job_data.setdefault('description', 'èŒä½æè¿°æš‚æ— ')
            job_data.setdefault('requirements', 'èŒä½è¦æ±‚æš‚æ— ')
            
            return job_data
            
        except Exception as e:
            self.logger.error(f"Playwrightæå–èŒä½æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _extract_generic_job_data(self, soup: BeautifulSoup, url: str) -> Optional[Dict[str, Any]]:
        """é€šç”¨èŒä½æ•°æ®æå–"""
        try:
            import hashlib
            job_data = {
                'id': hashlib.md5(url.encode()).hexdigest()[:8],
                'title': 'æœªçŸ¥èŒä½',
                'company': 'æœªçŸ¥å…¬å¸',
                'description': 'èŒä½æè¿°æš‚æ— ',
                'requirements': 'èŒä½è¦æ±‚æš‚æ— ',
                'tags': [],
                'company_info': {},
                'contact_info': {},
                'job_type': 'å…¨èŒ'
            }
            
            # å°è¯•ä»é¡µé¢æ ‡é¢˜æå–ä¿¡æ¯
            title_elem = soup.find('title')
            if title_elem:
                title_text = title_elem.get_text()
                job_data['title'] = title_text.split('-')[0].strip() if '-' in title_text else title_text
            
            return job_data
            
        except Exception as e:
            self.logger.error(f"é€šç”¨æ•°æ®æå–å¤±è´¥: {e}")
            return None


def create_scraper(site: str = 'boss', use_playwright: bool = True, 
                  headless: bool = False, user_data_dir: Optional[str] = None) -> JobScraper:
    """åˆ›å»ºçˆ¬è™«å®ä¾‹
    
    Args:
        site: ç½‘ç«™ç±»å‹ ('boss', 'generic')
        use_playwright: æ˜¯å¦ä½¿ç”¨Playwrightï¼ˆé»˜è®¤Trueï¼‰
        headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼Œæœ‰å¤´æ¨¡å¼æ›´éš¾è¢«æ£€æµ‹ï¼‰
        user_data_dir: ç”¨æˆ·æ•°æ®ç›®å½•è·¯å¾„ï¼Œç”¨äºä¿æŒç™»å½•çŠ¶æ€
        
    Returns:
        JobScraper: çˆ¬è™«å®ä¾‹
    """
    if use_playwright and HAS_PLAYWRIGHT_SUPPORT:
        return PlaywrightScraper(headless=headless, user_data_dir=user_data_dir)
    elif site.lower() == 'boss':
        return BossZhipinScraper()
    else:
        return JobScraper()


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    scraper = create_scraper('boss')
    
    # æµ‹è¯•å•ä¸ªèŒä½çˆ¬å–
    test_url = "https://www.zhipin.com/job_detail/xxx.html"
    result = scraper.scrape_job(test_url)
    
    if result.success:
        print(f"çˆ¬å–æˆåŠŸ: {result.job.title} @ {result.job.company}")
    else:
        print(f"çˆ¬å–å¤±è´¥: {result.error}")