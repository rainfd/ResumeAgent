# Resume Assistant å¢å¼ºçˆ¬è™«æ¨¡å—ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£ä»‹ç»äº† Resume Assistant ä¸­å¢å¼ºçš„ç½‘é¡µçˆ¬è™«æ¨¡å—ï¼Œè¯¥æ¨¡å—æ”¯æŒå¤šä¸ªæ‹›è˜ç½‘ç«™çš„èŒä½ä¿¡æ¯çˆ¬å–ï¼Œå…·å¤‡å…ˆè¿›çš„åæ£€æµ‹æœºåˆ¶ã€æ€§èƒ½ç›‘æ§å’Œé”™è¯¯å¤„ç†åŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹æ€§

### ğŸŒ å¤šç«™ç‚¹æ”¯æŒ
- **BOSSç›´è˜** (zhipin.com) - å®Œå…¨æ”¯æŒ
- **æ‹‰å‹¾ç½‘** (lagou.com) - æ–°å¢æ”¯æŒ
- **æ™ºè”æ‹›è˜** (zhaopin.com) - è®¡åˆ’ä¸­
- **çŒè˜ç½‘** (liepin.com) - è®¡åˆ’ä¸­
- **å‰ç¨‹æ— å¿§** (51job.com) - è®¡åˆ’ä¸­

### ğŸ›¡ï¸ åæ£€æµ‹æœºåˆ¶
- æ™ºèƒ½ç”¨æˆ·ä»£ç†è½®æ¢
- åŠ¨æ€å»¶è¿Ÿè®¡ç®—
- äººç±»è¡Œä¸ºæ¨¡æ‹Ÿ
- IPéªŒè¯å¤„ç†
- è¯·æ±‚é¢‘ç‡æ§åˆ¶

### ğŸ“Š æ€§èƒ½ç›‘æ§
- å®æ—¶çˆ¬å–ç»Ÿè®¡
- æˆåŠŸç‡ç›‘æ§
- å“åº”æ—¶é—´è¿½è¸ª
- æŒ‰ç½‘ç«™åˆ†ç±»ç»Ÿè®¡
- æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ

### ğŸ”§ é…ç½®ç®¡ç†
- çµæ´»çš„çˆ¬è™«é…ç½®
- å¹¶å‘æ§åˆ¶
- é‡è¯•æœºåˆ¶
- æ•°æ®éªŒè¯
- è¶…æ—¶è®¾ç½®

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from src.resume_assistant.core.scraping_orchestrator import scrape_job_url

# çˆ¬å–å•ä¸ªèŒä½
result = await scrape_job_url("https://www.zhipin.com/job_detail/123456.html")

if result.success:
    print(f"æˆåŠŸçˆ¬å–: {result.job.title} - {result.job.company}")
else:
    print(f"çˆ¬å–å¤±è´¥: {result.error}")
```

### æ‰¹é‡çˆ¬å–

```python
from src.resume_assistant.core.scraping_orchestrator import scrape_job_urls

urls = [
    "https://www.zhipin.com/job_detail/123456.html",
    "https://www.lagou.com/jobs/789012.html"
]

results = await scrape_job_urls(urls)

for result in results:
    if result.success:
        print(f"âœ… {result.job.title}")
    else:
        print(f"âŒ {result.url}: {result.error}")
```

### è‡ªå®šä¹‰é…ç½®

```python
from src.resume_assistant.core.scraping_orchestrator import (
    ScrapingOrchestrator, get_scraping_config
)

# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = get_scraping_config(
    headless=False,          # æ˜¾ç¤ºæµè§ˆå™¨çª—å£
    max_retries=5,           # æœ€å¤§é‡è¯•5æ¬¡
    concurrent_limit=2,      # å¹¶å‘é™åˆ¶2ä¸ª
    enable_monitoring=True   # å¯ç”¨æ€§èƒ½ç›‘æ§
)

# ä½¿ç”¨é…ç½®åˆ›å»ºåè°ƒå™¨
orchestrator = ScrapingOrchestrator(config)

# çˆ¬å–èŒä½
result = await orchestrator.scrape_single_job(url)
```

## è¯¦ç»†APIæ–‡æ¡£

### ScrapingOrchestrator

ä¸»è¦çš„çˆ¬è™«åè°ƒå™¨ç±»ï¼Œæä¾›ç»Ÿä¸€çš„çˆ¬è™«æ¥å£ã€‚

#### æ–¹æ³•

##### `scrape_single_job(url: str) -> ScrapingResult`
çˆ¬å–å•ä¸ªèŒä½URL

**å‚æ•°:**
- `url`: èŒä½é¡µé¢URL

**è¿”å›:** `ScrapingResult` å¯¹è±¡

**ç¤ºä¾‹:**
```python
orchestrator = ScrapingOrchestrator()
result = await orchestrator.scrape_single_job(url)
```

##### `scrape_multiple_jobs(urls: List[str]) -> List[ScrapingResult]`
å¹¶å‘çˆ¬å–å¤šä¸ªèŒä½URL

**å‚æ•°:**
- `urls`: èŒä½URLåˆ—è¡¨

**è¿”å›:** `ScrapingResult` å¯¹è±¡åˆ—è¡¨

**ç¤ºä¾‹:**
```python
results = await orchestrator.scrape_multiple_jobs(urls)
```

##### `get_supported_sites() -> List[str]`
è·å–æ”¯æŒçš„æ‹›è˜ç½‘ç«™åˆ—è¡¨

##### `is_url_supported(url: str) -> bool`
æ£€æŸ¥URLæ˜¯å¦è¢«æ”¯æŒ

##### `get_performance_stats() -> Dict[str, Any]`
è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯

##### `health_check() -> Dict[str, Any]`
æ‰§è¡Œå¥åº·æ£€æŸ¥

### ScrapingConfig

çˆ¬è™«é…ç½®ç±»ï¼Œç”¨äºè‡ªå®šä¹‰çˆ¬è™«è¡Œä¸ºã€‚

#### å‚æ•°

- `max_retries: int = 3` - æœ€å¤§é‡è¯•æ¬¡æ•°
- `retry_delay: float = 2.0` - é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
- `timeout: int = 30` - è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
- `concurrent_limit: int = 3` - å¹¶å‘é™åˆ¶
- `use_proxy: bool = False` - æ˜¯å¦ä½¿ç”¨ä»£ç†
- `proxy_pool: List[str] = []` - ä»£ç†æ± 
- `enable_monitoring: bool = True` - æ˜¯å¦å¯ç”¨ç›‘æ§
- `data_validation: bool = True` - æ˜¯å¦å¯ç”¨æ•°æ®éªŒè¯
- `headless: bool = False` - æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
- `user_data_dir: Optional[str] = None` - ç”¨æˆ·æ•°æ®ç›®å½•

### ScrapingResult

çˆ¬å–ç»“æœç±»ï¼ŒåŒ…å«çˆ¬å–çš„çŠ¶æ€å’Œæ•°æ®ã€‚

#### å±æ€§

- `success: bool` - æ˜¯å¦æˆåŠŸ
- `job: Optional[Job]` - èŒä½å¯¹è±¡ï¼ˆæˆåŠŸæ—¶ï¼‰
- `error: Optional[str]` - é”™è¯¯ä¿¡æ¯ï¼ˆå¤±è´¥æ—¶ï¼‰
- `url: Optional[str]` - åŸå§‹URL
- `scraped_at: Optional[datetime]` - çˆ¬å–æ—¶é—´

## Streamlit Webç•Œé¢é›†æˆ

### EnhancedJobAdapter

å¢å¼ºçš„èŒä½ç®¡ç†Webé€‚é…å™¨ï¼Œæä¾›Streamlitå‹å¥½çš„æ¥å£ã€‚

```python
from src.resume_assistant.web.enhanced_job_adapter import EnhancedJobAdapter

# åˆ›å»ºé€‚é…å™¨
adapter = EnhancedJobAdapter(db_manager)

# åŒæ­¥çˆ¬å–ï¼ˆStreamlitå…¼å®¹ï¼‰
result = adapter.scrape_single_job_sync(url, config={
    'headless': False,
    'max_retries': 3
})

# æ‰¹é‡çˆ¬å–
results = adapter.scrape_multiple_jobs_sync(urls)

# è·å–æ”¯æŒçš„ç½‘ç«™
sites = adapter.get_supported_sites()

# æ€§èƒ½ç»Ÿè®¡
stats = adapter.get_performance_stats()
```

### UI åŠ©æ‰‹å‡½æ•°

```python
from src.resume_assistant.web.enhanced_job_adapter import (
    create_scraping_config_from_ui,
    display_scraping_result,
    display_batch_results,
    display_performance_stats
)

# ä»UIåˆ›å»ºé…ç½®
config = create_scraping_config_from_ui()

# æ˜¾ç¤ºçˆ¬å–ç»“æœ
display_scraping_result(result)

# æ˜¾ç¤ºæ‰¹é‡ç»“æœ
display_batch_results(results)

# æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
display_performance_stats(stats)
```

## å…·ä½“ç½‘ç«™ä½¿ç”¨

### BOSSç›´è˜

**æ”¯æŒçš„URLæ ¼å¼:**
- `https://www.zhipin.com/job_detail/123456.html`

**ç‰¹æ€§:**
- å®Œæ•´çš„åæ£€æµ‹æœºåˆ¶
- IPéªŒè¯è‡ªåŠ¨å¤„ç†
- å®Œæ•´çš„èŒä½ä¿¡æ¯æå–

**ä½¿ç”¨ç¤ºä¾‹:**
```python
url = "https://www.zhipin.com/job_detail/123456.html"
result = await scrape_job_url(url)
```

### æ‹‰å‹¾ç½‘

**æ”¯æŒçš„URLæ ¼å¼:**
- `https://www.lagou.com/jobs/123456.html`
- `https://www.lagou.com/jobs/123456`

**ç‰¹æ€§:**
- é’ˆå¯¹æ‹‰å‹¾ç½‘çš„ä¸“é—¨ä¼˜åŒ–
- æ™ºèƒ½æŠ€èƒ½æå–
- å…¬å¸ä¿¡æ¯è§£æ

**ä½¿ç”¨ç¤ºä¾‹:**
```python
from src.resume_assistant.core.lagou_scraper import scrape_lagou_job

url = "https://www.lagou.com/jobs/123456.html"
result = scrape_lagou_job(url, headless=False)
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å¹¶å‘æ§åˆ¶

æ ¹æ®ç½‘ç«™çš„æ‰¿å—èƒ½åŠ›è°ƒæ•´å¹¶å‘æ•°é‡ï¼š
- BOSSç›´è˜ï¼šå»ºè®®å¹¶å‘ä¸è¶…è¿‡3
- æ‹‰å‹¾ç½‘ï¼šå»ºè®®å¹¶å‘ä¸è¶…è¿‡2

```python
config = ScrapingConfig(concurrent_limit=2)
```

### 2. å»¶è¿Ÿè®¾ç½®

é€‚å½“çš„å»¶è¿Ÿå¯ä»¥é¿å…è¢«æ£€æµ‹ï¼š
- é«˜é¢‘çˆ¬å–ï¼š1-3ç§’å»¶è¿Ÿ
- å¸¸è§„çˆ¬å–ï¼š2-5ç§’å»¶è¿Ÿ

```python
config = ScrapingConfig(retry_delay=3.0)
```

### 3. é”™è¯¯å¤„ç†

è®¾ç½®åˆç†çš„é‡è¯•æ¬¡æ•°ï¼š
- ç½‘ç»œä¸ç¨³å®šï¼šå¢åŠ é‡è¯•æ¬¡æ•°
- ç›®æ ‡ç½‘ç«™ç¨³å®šï¼šå‡å°‘é‡è¯•æ¬¡æ•°

```python
config = ScrapingConfig(max_retries=5)
```

### 4. èµ„æºç®¡ç†

åŠæ—¶æ¸…ç†æµè§ˆå™¨èµ„æºï¼š

```python
orchestrator = ScrapingOrchestrator()
try:
    result = await orchestrator.scrape_single_job(url)
finally:
    orchestrator.cleanup()  # é‡è¦ï¼šæ¸…ç†èµ„æº
```

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

#### 1. ç½‘ç»œè¿æ¥é”™è¯¯
```
é”™è¯¯: ç½‘ç»œè¿æ¥è¶…æ—¶
è§£å†³: å¢åŠ è¶…æ—¶æ—¶é—´æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥
```

#### 2. åçˆ¬éªŒè¯
```
é”™è¯¯: æ£€æµ‹åˆ°äººæœºéªŒè¯
è§£å†³: ä½¿ç”¨æœ‰å¤´æ¨¡å¼ï¼Œæ‰‹åŠ¨å®ŒæˆéªŒè¯
```

#### 3. é¡µé¢ç»“æ„å˜åŒ–
```
é”™è¯¯: æ— æ³•æå–èŒä½ä¿¡æ¯
è§£å†³: æ£€æŸ¥ç›®æ ‡ç½‘ç«™æ˜¯å¦æ›´æ–°äº†é¡µé¢ç»“æ„
```

#### 4. æµè§ˆå™¨å¯åŠ¨å¤±è´¥
```
é”™è¯¯: Playwrightæµè§ˆå™¨å¯åŠ¨å¤±è´¥
è§£å†³: ç¡®ä¿å·²å®‰è£…playwrightæµè§ˆå™¨
```

å®‰è£…å‘½ä»¤ï¼š
```bash
pip install playwright
playwright install chromium
```

### è°ƒè¯•æŠ€å·§

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### 2. ä½¿ç”¨æœ‰å¤´æ¨¡å¼

```python
config = ScrapingConfig(headless=False)
```

#### 3. ä¿å­˜ç”¨æˆ·æ•°æ®

```python
config = ScrapingConfig(user_data_dir="./browser_data")
```

## æ€§èƒ½ç›‘æ§

### å¯ç”¨ç›‘æ§

```python
config = ScrapingConfig(enable_monitoring=True)
orchestrator = ScrapingOrchestrator(config)

# è·å–ç»Ÿè®¡ä¿¡æ¯
stats = orchestrator.get_performance_stats()
```

### ç›‘æ§æŒ‡æ ‡

- **æ€»å°è¯•æ¬¡æ•°**: æ‰€æœ‰çˆ¬å–å°è¯•çš„æ€»æ•°
- **æˆåŠŸç‡**: æˆåŠŸçˆ¬å–çš„ç™¾åˆ†æ¯”
- **å¹³å‡å“åº”æ—¶é—´**: çˆ¬å–çš„å¹³å‡è€—æ—¶
- **æŒ‰ç½‘ç«™ç»Ÿè®¡**: æ¯ä¸ªç½‘ç«™çš„è¯¦ç»†ç»Ÿè®¡

### æ€§èƒ½æŠ¥å‘Šç¤ºä¾‹

```json
{
  "overall": {
    "total_attempts": 100,
    "success_rate": "85.00%",
    "average_response_time": "3.25s",
    "successful_scrapes": 85,
    "failed_scrapes": 15
  },
  "by_site": {
    "boss": {
      "attempts": 60,
      "success_rate": "90.00%",
      "avg_response_time": "2.80s",
      "last_success": "2024-01-15T10:30:00",
      "last_failure": "2024-01-15T09:15:00"
    },
    "lagou": {
      "attempts": 40,
      "success_rate": "77.50%",
      "avg_response_time": "3.90s",
      "last_success": "2024-01-15T10:25:00",
      "last_failure": "2024-01-15T10:20:00"
    }
  }
}
```

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°ç½‘ç«™æ”¯æŒ

1. **åˆ›å»ºçˆ¬è™«ç±»**

```python
from src.resume_assistant.core.scraper import JobScraper

class NewSiteScraper(JobScraper):
    def scrape_job(self, url: str) -> ScrapingResult:
        # å®ç°çˆ¬å–é€»è¾‘
        pass
```

2. **æ³¨å†Œåˆ°åè°ƒå™¨**

```python
# åœ¨ MultiSiteScraper._initialize_scrapers() ä¸­æ·»åŠ 
self.scrapers['newsite'] = NewSiteScraper()
```

3. **æ›´æ–°ç½‘ç«™æ£€æµ‹**

```python
# åœ¨ MultiSiteScraper.detect_site() ä¸­æ·»åŠ 
elif 'newsite.com' in domain:
    return SiteSupport.NEWSITE
```

### è‡ªå®šä¹‰åæ£€æµ‹ç­–ç•¥

```python
class CustomAntiDetection(AntiDetectionManager):
    def calculate_delay(self) -> float:
        # è‡ªå®šä¹‰å»¶è¿Ÿç®—æ³•
        return super().calculate_delay() * 1.5
```

## æœ€ä½³å®è·µ

### 1. é…ç½®ç®¡ç†

```python
# ç”Ÿäº§ç¯å¢ƒé…ç½®
PRODUCTION_CONFIG = ScrapingConfig(
    headless=True,
    max_retries=3,
    concurrent_limit=2,
    enable_monitoring=True,
    data_validation=True
)

# å¼€å‘ç¯å¢ƒé…ç½®
DEVELOPMENT_CONFIG = ScrapingConfig(
    headless=False,
    max_retries=1,
    concurrent_limit=1,
    enable_monitoring=False
)
```

### 2. é”™è¯¯å¤„ç†

```python
async def safe_scrape(url: str) -> Optional[Job]:
    try:
        result = await scrape_job_url(url)
        if result.success:
            return result.job
        else:
            logger.error(f"çˆ¬å–å¤±è´¥: {result.error}")
    except Exception as e:
        logger.exception(f"çˆ¬å–å¼‚å¸¸: {e}")
    return None
```

### 3. æ‰¹é‡å¤„ç†

```python
async def batch_scrape_with_progress(urls: List[str]):
    results = []
    
    for i, url in enumerate(urls):
        result = await scrape_job_url(url)
        results.append(result)
        
        # æ˜¾ç¤ºè¿›åº¦
        progress = (i + 1) / len(urls)
        print(f"è¿›åº¦: {progress:.1%}")
        
        # é¿å…è¿‡å¿«è¯·æ±‚
        await asyncio.sleep(2)
    
    return results
```

## æ•…éšœæ’é™¤

### é—®é¢˜è¯Šæ–­æ­¥éª¤

1. **æ£€æŸ¥ä¾èµ–å®‰è£…**
```bash
pip install playwright beautifulsoup4 requests
playwright install
```

2. **éªŒè¯URLæ ¼å¼**
```python
orchestrator = ScrapingOrchestrator()
print(orchestrator.is_url_supported(url))
```

3. **æ£€æŸ¥ç½‘ç»œè¿æ¥**
```python
health = await orchestrator.health_check()
print(health)
```

4. **æŸ¥çœ‹è¯¦ç»†æ—¥å¿—**
```python
import logging
logging.getLogger('src.resume_assistant.core').setLevel(logging.DEBUG)
```

### å¸¸è§é—®é¢˜è§£ç­”

**Q: ä¸ºä»€ä¹ˆçˆ¬å–é€Ÿåº¦å¾ˆæ…¢ï¼Ÿ**
A: è¿™æ˜¯ä¸ºäº†é¿å…è¢«åçˆ¬æœºåˆ¶æ£€æµ‹ã€‚å¯ä»¥é€‚å½“è°ƒæ•´å»¶è¿Ÿæ—¶é—´ï¼Œä½†ä¸å»ºè®®è®¾ç½®è¿‡å¿«ã€‚

**Q: å¦‚ä½•å¤„ç†éªŒè¯ç ï¼Ÿ**
A: ä½¿ç”¨æœ‰å¤´æ¨¡å¼ï¼ˆheadless=Falseï¼‰ï¼Œç³»ç»Ÿä¼šæš‚åœç­‰å¾…æ‰‹åŠ¨å¤„ç†éªŒè¯ç ã€‚

**Q: æ”¯æŒä»£ç†å—ï¼Ÿ**
A: ç›®å‰ä»£ç†åŠŸèƒ½åœ¨å¼€å‘ä¸­ï¼Œå¯ä»¥é€šè¿‡é…ç½®å¯ç”¨ï¼ˆä½†éœ€è¦è‡ªè¡Œå®ç°ä»£ç†é€»è¾‘ï¼‰ã€‚

**Q: å¦‚ä½•æé«˜æˆåŠŸç‡ï¼Ÿ**
A: 1) ä½¿ç”¨æœ‰å¤´æ¨¡å¼ 2) å¢åŠ é‡è¯•æ¬¡æ•° 3) é€‚å½“å¢åŠ å»¶è¿Ÿæ—¶é—´ 4) ä¿æŒæµè§ˆå™¨çŠ¶æ€

**Q: æ•°æ®æå–ä¸å‡†ç¡®æ€ä¹ˆåŠï¼Ÿ**
A: ç›®æ ‡ç½‘ç«™å¯èƒ½æ›´æ–°äº†é¡µé¢ç»“æ„ï¼Œéœ€è¦æ›´æ–°å¯¹åº”çš„çˆ¬è™«ä»£ç ã€‚

## è”ç³»æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜æˆ–éœ€è¦æ·»åŠ æ–°ç½‘ç«™æ”¯æŒï¼Œè¯·ï¼š

1. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
2. æ£€æŸ¥ç½‘ç«™æ˜¯å¦æœ‰ç»“æ„å˜åŒ–
3. æäº¤é—®é¢˜åé¦ˆï¼ŒåŒ…å«è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’ŒURLç¤ºä¾‹

---

**æ³¨æ„**: ä½¿ç”¨çˆ¬è™«æ—¶è¯·éµå®ˆç›®æ ‡ç½‘ç«™çš„ robots.txt å’Œä½¿ç”¨æ¡æ¬¾ï¼Œåˆç†æ§åˆ¶çˆ¬å–é¢‘ç‡ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆè¿‡å¤§è´Ÿæ‹…ã€‚