#!/usr/bin/env python3
"""
çˆ¬è™«æ¨¡å—æµ‹è¯•è„šæœ¬

æµ‹è¯•Playwrightå’Œrequestsä¸¤ç§çˆ¬å–æ–¹å¼å¯¹BOSSç›´è˜èŒä½é¡µé¢çš„çˆ¬å–æ•ˆæœ
"""

import sys
import os
import asyncio
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))

from resume_assistant.core.scraper import (
    create_scraper, 
    JobScraper, 
    PlaywrightScraper, 
    BossZhipinScraper,
    ScrapingResult
)
from resume_assistant.utils import get_logger

# è®¾ç½®æ—¥å¿—
logger = get_logger(__name__)

class ScraperTestSuite:
    """çˆ¬è™«æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.test_url = "https://www.zhipin.com/job_detail/29090ef211bc5eeb03Fz3dq0GFJT.html?securityId=bJzOfMuGJ2F22-61vKFksunOnKBpKMBvBxLCosBmd6oKgS-VKgAqEw7kDzXQdLe4VeIEz6q1iod1iRUkjA1tp9iHqBweo0KuhI9goLnJv-Rbe1Cs96KMmw~~&ka=personal_added_job_29090ef211bc5eeb03Fz3dq0GFJT"
        self.results = {}
        
    def print_header(self, title: str):
        """æ‰“å°æµ‹è¯•æ ‡é¢˜"""
        print("\n" + "="*60)
        print(f"  {title}")
        print("="*60)
    
    def print_result(self, result: ScrapingResult, scraper_name: str):
        """æ‰“å°çˆ¬å–ç»“æœ"""
        print(f"\nğŸ“Š {scraper_name} çˆ¬å–ç»“æœ:")
        print(f"   çŠ¶æ€: {'âœ… æˆåŠŸ' if result.success else 'âŒ å¤±è´¥'}")
        
        if result.success and result.job:
            job = result.job
            print(f"   èŒä½ID: {job.id}")
            print(f"   èŒä½æ ‡é¢˜: {job.title}")
            print(f"   å…¬å¸åç§°: {job.company}")
            print(f"   å·¥ä½œåœ°ç‚¹: {job.location}")
            print(f"   è–ªèµ„èŒƒå›´: {job.salary}")
            print(f"   ç»éªŒè¦æ±‚: {job.experience_level}")
            print(f"   å­¦å†è¦æ±‚: {job.education_level}")
            print(f"   èŒä½ç±»å‹: {job.job_type}")
            print(f"   èŒä½æ ‡ç­¾: {', '.join(job.tags) if job.tags else 'æ— '}")
            print(f"   èŒä½æè¿°: {job.description[:100]}..." if len(job.description) > 100 else f"   èŒä½æè¿°: {job.description}")
            print(f"   çˆ¬å–æ—¶é—´: {result.scraped_at}")
        else:
            print(f"   é”™è¯¯ä¿¡æ¯: {result.error}")
            
        print(f"   æ¥æºURL: {result.url}")
    
    def test_playwright_scraper(self):
        """æµ‹è¯•Playwrightçˆ¬è™«"""
        self.print_header("æµ‹è¯• Playwright çˆ¬è™«")
        
        try:
            print("ğŸš€ åˆå§‹åŒ–Playwrightçˆ¬è™«...")
            scraper = PlaywrightScraper()
            
            print(f"ğŸŒ å¼€å§‹çˆ¬å–URL: {self.test_url}")
            start_time = time.time()
            
            result = scraper.scrape_job(self.test_url)
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            print(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f}ç§’")
            
            self.print_result(result, "Playwright")
            self.results['playwright'] = {
                'result': result,
                'elapsed_time': elapsed_time,
                'scraper_type': 'Playwright'
            }
            
        except Exception as e:
            error_msg = f"Playwrightçˆ¬è™«æµ‹è¯•å¼‚å¸¸: {e}"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            self.results['playwright'] = {
                'result': ScrapingResult(success=False, error=error_msg, url=self.test_url),
                'elapsed_time': 0,
                'scraper_type': 'Playwright'
            }
    
    def test_requests_scraper(self):
        """æµ‹è¯•requestsçˆ¬è™«"""
        self.print_header("æµ‹è¯• Requests + BeautifulSoup çˆ¬è™«")
        
        try:
            print("ğŸš€ åˆå§‹åŒ–BossZhipinçˆ¬è™«...")
            scraper = BossZhipinScraper()
            
            print(f"ğŸŒ å¼€å§‹çˆ¬å–URL: {self.test_url}")
            start_time = time.time()
            
            result = scraper.scrape_job(self.test_url)
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            print(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f}ç§’")
            
            self.print_result(result, "Requests")
            self.results['requests'] = {
                'result': result,
                'elapsed_time': elapsed_time,
                'scraper_type': 'Requests+BeautifulSoup'
            }
            
        except Exception as e:
            error_msg = f"Requestsçˆ¬è™«æµ‹è¯•å¼‚å¸¸: {e}"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            self.results['requests'] = {
                'result': ScrapingResult(success=False, error=error_msg, url=self.test_url),
                'elapsed_time': 0,
                'scraper_type': 'Requests+BeautifulSoup'
            }
    
    def test_generic_scraper(self):
        """æµ‹è¯•é€šç”¨çˆ¬è™«"""
        self.print_header("æµ‹è¯• é€šç”¨çˆ¬è™«")
        
        try:
            print("ğŸš€ åˆå§‹åŒ–é€šç”¨çˆ¬è™«...")
            scraper = JobScraper()
            
            print(f"ğŸŒ å¼€å§‹çˆ¬å–URL: {self.test_url}")
            start_time = time.time()
            
            result = scraper.scrape_job(self.test_url)
            
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            print(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f}ç§’")
            
            self.print_result(result, "Generic")
            self.results['generic'] = {
                'result': result,
                'elapsed_time': elapsed_time,
                'scraper_type': 'Generic'
            }
            
        except Exception as e:
            error_msg = f"é€šç”¨çˆ¬è™«æµ‹è¯•å¼‚å¸¸: {e}"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            self.results['generic'] = {
                'result': ScrapingResult(success=False, error=error_msg, url=self.test_url),
                'elapsed_time': 0,
                'scraper_type': 'Generic'
            }
    
    def compare_results(self):
        """å¯¹æ¯”æµ‹è¯•ç»“æœ"""
        self.print_header("çˆ¬è™«æ€§èƒ½å¯¹æ¯”åˆ†æ")
        
        if not self.results:
            print("âŒ æ²¡æœ‰æµ‹è¯•ç»“æœå¯ä¾›å¯¹æ¯”")
            return
        
        print("\nğŸ“ˆ æµ‹è¯•ç»“æœæ±‡æ€»:")
        print(f"{'çˆ¬è™«ç±»å‹':^20} | {'çŠ¶æ€':^8} | {'è€—æ—¶(ç§’)':^10} | {'æ•°æ®å®Œæ•´æ€§':^12}")
        print("-" * 60)
        
        success_count = 0
        total_time = 0
        
        for name, data in self.results.items():
            result = data['result']
            elapsed = data['elapsed_time']
            scraper_type = data['scraper_type']
            
            status = "âœ… æˆåŠŸ" if result.success else "âŒ å¤±è´¥"
            
            # è¯„ä¼°æ•°æ®å®Œæ•´æ€§
            completeness = "æ— æ•°æ®"
            if result.success and result.job:
                job = result.job
                fields = [job.title, job.company, job.description, job.salary, job.location]
                filled_fields = sum(1 for field in fields if field and field.strip())
                completeness = f"{filled_fields}/5"
                if filled_fields >= 4:
                    completeness += " ä¼˜ç§€"
                elif filled_fields >= 2:
                    completeness += " ä¸€èˆ¬"
                else:
                    completeness += " è¾ƒå·®"
            
            print(f"{scraper_type:^20} | {status:^8} | {elapsed:^10.2f} | {completeness:^12}")
            
            if result.success:
                success_count += 1
            total_time += elapsed
        
        print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æˆåŠŸç‡: {success_count}/{len(self.results)} ({success_count/len(self.results)*100:.1f}%)")
        print(f"   å¹³å‡è€—æ—¶: {total_time/len(self.results):.2f}ç§’")
        
        # æ¨èæœ€ä½³çˆ¬è™«
        best_scraper = None
        best_score = 0
        
        for name, data in self.results.items():
            result = data['result']
            elapsed = data['elapsed_time']
            
            # è®¡ç®—ç»¼åˆè¯„åˆ† (æˆåŠŸ=100åˆ†, é€Ÿåº¦åŠ åˆ†, æ•°æ®å®Œæ•´æ€§åŠ åˆ†)
            score = 0
            if result.success:
                score += 100
                # é€Ÿåº¦åŠ åˆ† (è¶Šå¿«è¶Šå¥½ï¼Œæœ€å¤š20åˆ†)
                if elapsed > 0:
                    speed_score = max(0, 20 - elapsed)
                    score += speed_score
                
                # æ•°æ®å®Œæ•´æ€§åŠ åˆ†
                if result.job:
                    job = result.job
                    fields = [job.title, job.company, job.description, job.salary, job.location]
                    filled_fields = sum(1 for field in fields if field and field.strip())
                    score += filled_fields * 4  # æ¯ä¸ªå­—æ®µ4åˆ†
            
            if score > best_score:
                best_score = score
                best_scraper = data['scraper_type']
        
        if best_scraper:
            print(f"\nğŸ† æ¨èçˆ¬è™«: {best_scraper} (ç»¼åˆè¯„åˆ†: {best_score:.1f})")
        
        # é”™è¯¯åˆ†æ
        print("\nğŸ” é”™è¯¯åˆ†æ:")
        for name, data in self.results.items():
            result = data['result']
            if not result.success:
                print(f"   {data['scraper_type']}: {result.error}")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ¯ Resume Assistant çˆ¬è™«æ¨¡å—æµ‹è¯•")
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æµ‹è¯•URL: {self.test_url}")
        
        # ä¾æ¬¡æµ‹è¯•å„ç§çˆ¬è™«
        self.test_playwright_scraper()
        time.sleep(2)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        
        self.test_requests_scraper()
        time.sleep(2)
        
        self.test_generic_scraper()
        
        # å¯¹æ¯”åˆ†æ
        self.compare_results()
        
        self.print_header("æµ‹è¯•å®Œæˆ")
        print("ğŸ’¡ å»ºè®®:")
        print("   1. å¦‚æœPlaywrightæˆåŠŸï¼Œä¼˜å…ˆä½¿ç”¨Playwrightçˆ¬è™«")
        print("   2. å¦‚æœéƒ½å¤±è´¥ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨æ·»åŠ èŒä½ä¿¡æ¯")
        print("   3. å¯¹äºBOSSç›´è˜ç­‰åçˆ¬ç½‘ç«™ï¼Œå»ºè®®ç»“åˆå¤šç§ç­–ç•¥")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æ£€æŸ¥ä¾èµ–åº“...")
    
    # æ£€æŸ¥ä¾èµ–
    missing_deps = []
    
    try:
        import requests
        import bs4
        print("âœ… requests + beautifulsoup4 å¯ç”¨")
    except ImportError:
        missing_deps.append("requests beautifulsoup4")
        print("âŒ requests + beautifulsoup4 ä¸å¯ç”¨")
    
    try:
        import playwright
        print("âœ… playwright å¯ç”¨")
    except ImportError:
        missing_deps.append("playwright")
        print("âŒ playwright ä¸å¯ç”¨")
    
    if missing_deps:
        print(f"\nâš ï¸  ç¼ºå°‘ä¾èµ–: {', '.join(missing_deps)}")
        print("è¯·å®‰è£…: pip install " + " ".join(missing_deps))
        return
    
    # è¿è¡Œæµ‹è¯•
    test_suite = ScraperTestSuite()
    test_suite.run_all_tests()

if __name__ == "__main__":
    main()