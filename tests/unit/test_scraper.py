"""ç½‘é¡µçˆ¬è™«å•å…ƒæµ‹è¯• - ä½¿ç”¨çœŸå®æ•°æ®"""

import unittest
import asyncio
import sys
import json
from pathlib import Path
from unittest.mock import patch, MagicMock, AsyncMock
from datetime import datetime

# æ·»åŠ srcè·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from resume_assistant.core.scraper import PlaywrightScraper, BossZhipinScraper, JobScraper, ScrapingResult
from resume_assistant.core.job_manager import Job
from resume_assistant.utils.errors import NetworkError, ParseError as ScrapingError


# åŠ è½½çœŸå®æ•°æ®
def load_real_job_urls():
    """åŠ è½½çœŸå®èŒä½URLæ•°æ®"""
    project_root = Path(__file__).parent.parent.parent
    jobs_metadata_file = project_root / "data" / "jobs" / "jobs_metadata.json"
    if jobs_metadata_file.exists():
        with open(jobs_metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
            # æå–URLä¿¡æ¯ï¼Œè™½ç„¶å½“å‰æ•°æ®ä¸­URLä¸ºnullï¼Œä½†æˆ‘ä»¬å¯ä»¥æ„é€ çœŸå®çš„æ‹‰å‹¾ç½‘URL
            real_urls = []
            for job_id, job_info in metadata.items():
                # åŸºäºçœŸå®èŒä½æ•°æ®æ„é€ æ‹‰å‹¾ç½‘URL
                title_en = job_info['title'].replace('å·¥ç¨‹å¸ˆ', 'engineer').replace('Python', 'python')
                real_url = f"https://www.lagou.com/jobs/{job_id}.html"
                real_urls.append({
                    'url': real_url,
                    'title': job_info['title'],
                    'company': job_info['company'],
                    'location': job_info['location'],
                    'id': job_id
                })
            return real_urls
    return []

def load_real_job_data():
    """åŠ è½½çœŸå®èŒä½è¯¦ç»†æ•°æ®"""
    project_root = Path(__file__).parent.parent.parent
    job_file = project_root / "data" / "jobs" / "5c384a14-4174-4c51-b5b9-87ef63454441.json"
    if job_file.exists():
        with open(job_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return None

def generate_real_html_content(job_data):
    """åŸºäºçœŸå®èŒä½æ•°æ®ç”ŸæˆHTMLå†…å®¹"""
    if not job_data:
        return ""
    
    return f"""
    <html>
        <head>
            <title>{job_data['title']} - {job_data['company']} - æ‹‰å‹¾ç½‘</title>
            <meta name="description" content="{job_data['description']}">
        </head>
        <body>
            <div class="position-content">
                <div class="position-head">
                    <h1 class="name">{job_data['title']}</h1>
                    <span class="salary">{job_data['salary']}</span>
                    <span class="experience">{job_data.get('experience_level', '3-5å¹´')}</span>
                </div>
                <p class="company_name">
                    <a href="/company/{job_data['id']}.html">{job_data['company']}</a>
                </p>
                <p class="work_addr">{job_data['location']}</p>
                <div class="job_bt">
                    <h3>èŒä½æè¿°</h3>
                    <p>{job_data['description']}</p>
                    <h3>èŒä½è¦æ±‚</h3>
                    <p>{job_data['requirements']}</p>
                </div>
                <div class="job-tags">
                    <span class="tag">Python</span>
                    <span class="tag">Django</span>
                    <span class="tag">MySQL</span>
                </div>
            </div>
            
            <!-- æ‹‰å‹¾ç½‘ç‰¹æœ‰çš„ç»“æ„ -->
            <div class="company-info">
                <h3>å…¬å¸ä¿¡æ¯</h3>
                <p class="company-stage">Aè½®</p>
                <p class="company-size">150-500äºº</p>
                <p class="company-industry">äº’è”ç½‘</p>
            </div>
        </body>
    </html>
    """


class TestScrapingResult(unittest.TestCase):
    """çˆ¬è™«ç»“æœæµ‹è¯• - ä½¿ç”¨çœŸå®æ•°æ®"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.real_job_data = load_real_job_data()
        self.real_urls = load_real_job_urls()
    
    def test_scraping_result_with_real_data(self):
        """æµ‹è¯•ä½¿ç”¨çœŸå®æ•°æ®åˆ›å»ºçˆ¬è™«ç»“æœ"""
        if not self.real_job_data or not self.real_urls:
            self.skipTest("çœŸå®æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        
        real_url_info = self.real_urls[0]
        
        # ä½¿ç”¨çœŸå®æ•°æ®åˆ›å»ºJobå¯¹è±¡
        job = Job(
            id=self.real_job_data['id'],
            title=self.real_job_data['title'],
            company=self.real_job_data['company'],
            description=self.real_job_data['description'],
            requirements=self.real_job_data['requirements'],
            location=self.real_job_data['location'],
            salary=self.real_job_data['salary'],
            source_url=real_url_info['url']
        )
        
        # åˆ›å»ºçˆ¬è™«ç»“æœ
        result = ScrapingResult(
            success=True,
            job=job,
            url=real_url_info['url'],
            scraped_at=datetime.now()
        )
        
        # éªŒè¯ç»“æœåŒ…å«çœŸå®æ•°æ®
        self.assertTrue(result.success)
        self.assertIsNotNone(result.job)
        self.assertEqual(result.job.title, "Pythonåç«¯å¼€å‘å·¥ç¨‹å¸ˆ")
        self.assertEqual(result.job.company, "ç§‘æŠ€æœ‰é™å…¬å¸A")
        self.assertEqual(result.job.location, "åŒ—äº¬")
        self.assertEqual(result.job.salary, "20-35K")
        self.assertIn("Python", result.job.requirements)
        self.assertIn("Django", result.job.requirements)
        self.assertTrue(result.url.startswith("https://www.lagou.com/jobs/"))
        self.assertIsInstance(result.scraped_at, datetime)
    
    def test_scraping_result_serialization(self):
        """æµ‹è¯•çˆ¬è™«ç»“æœåºåˆ—åŒ–"""
        if not self.real_job_data:
            self.skipTest("çœŸå®èŒä½æ•°æ®ä¸å­˜åœ¨")
        
        # åˆ›å»ºJobå¯¹è±¡ç”¨äºæµ‹è¯•
        job = Job(
            id="test-job-id",
            title=self.real_job_data['title'],
            company=self.real_job_data['company'],
            description=self.real_job_data['description'],
            requirements=self.real_job_data['requirements'],
            location=self.real_job_data['location'],
            salary=self.real_job_data['salary']
        )
        
        result = ScrapingResult(
            success=True,
            job=job,
            url="https://www.lagou.com/jobs/real-test.html",
            scraped_at=datetime.now()
        )
        
        # æµ‹è¯•åŸºæœ¬å±æ€§
        self.assertTrue(result.success)
        self.assertIsNotNone(result.job)
        self.assertEqual(result.job.title, self.real_job_data['title'])
        self.assertEqual(result.job.company, self.real_job_data['company'])
        
        # æµ‹è¯•å¯ä»¥è®¿é—®Jobçš„æ•°æ®
        self.assertIn("Python", result.job.requirements)
        self.assertIsNotNone(result.url)


class TestPlaywrightScraper(unittest.TestCase):
    """Playwrightçˆ¬è™«æµ‹è¯• - ä½¿ç”¨çœŸå®åœºæ™¯"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.scraper = PlaywrightScraper()
        self.real_job_data = load_real_job_data()
        self.real_urls = load_real_job_urls()
    
    @patch('playwright.async_api.async_playwright')
    async def test_scrape_with_real_html_structure(self, mock_playwright):
        """æµ‹è¯•ä½¿ç”¨çœŸå®HTMLç»“æ„çš„çˆ¬å–"""
        if not self.real_job_data or not self.real_urls:
            self.skipTest("çœŸå®æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è®¾ç½®Playwrightæ¨¡æ‹Ÿ
        mock_browser = AsyncMock()
        mock_page = AsyncMock()
        mock_context = AsyncMock()
        
        mock_context.new_page.return_value = mock_page
        mock_browser.new_context.return_value = mock_context
        
        mock_playwright_instance = AsyncMock()
        mock_playwright_instance.chromium.launch.return_value = mock_browser
        mock_playwright.return_value.__aenter__.return_value = mock_playwright_instance
        
        # ä½¿ç”¨çœŸå®æ•°æ®ç”ŸæˆHTMLå†…å®¹
        real_html = generate_real_html_content(self.real_job_data)
        mock_page.content.return_value = real_html
        
        # æ¨¡æ‹Ÿé¡µé¢å“åº”
        mock_page.goto.return_value = None
        mock_page.wait_for_load_state.return_value = None
        
        # æ‰§è¡Œçˆ¬å–
        real_url = self.real_urls[0]['url']
        result = await self.scraper.scrape(real_url)
        
        # éªŒè¯çˆ¬å–ç»“æœ
        self.assertIsNotNone(result)
        self.assertEqual(result.url, real_url)
        self.assertIsInstance(result, ScrapingResult)
        
        # éªŒè¯æµè§ˆå™¨æ“ä½œ
        mock_playwright_instance.chromium.launch.assert_called_once()
        mock_browser.new_context.assert_called_once()
        mock_page.goto.assert_called_once_with(real_url)
        mock_browser.close.assert_called_once()
    
    @patch('playwright.async_api.async_playwright')
    async def test_scrape_multiple_real_urls(self, mock_playwright):
        """æµ‹è¯•çˆ¬å–å¤šä¸ªçœŸå®URL"""
        if not self.real_urls:
            self.skipTest("çœŸå®URLæ•°æ®ä¸å­˜åœ¨")
        
        # è®¾ç½®Playwrightæ¨¡æ‹Ÿ
        mock_browser = AsyncMock()
        mock_page = AsyncMock()
        mock_context = AsyncMock()
        
        mock_context.new_page.return_value = mock_page
        mock_browser.new_context.return_value = mock_context
        
        mock_playwright_instance = AsyncMock()
        mock_playwright_instance.chromium.launch.return_value = mock_browser
        mock_playwright.return_value.__aenter__.return_value = mock_playwright_instance
        
        # ä¸ºæ¯ä¸ªURLç”Ÿæˆä¸åŒçš„HTMLå†…å®¹
        html_responses = []
        for url_info in self.real_urls[:3]:  # æµ‹è¯•å‰3ä¸ªURL
            html_content = f"""
            <html>
                <head><title>{url_info['title']} - {url_info['company']}</title></head>
                <body>
                    <h1 class="job-title">{url_info['title']}</h1>
                    <div class="company-name">{url_info['company']}</div>
                    <div class="location">{url_info['location']}</div>
                    <div class="job-desc">èŒä½æè¿°å†…å®¹...</div>
                </body>
            </html>
            """
            html_responses.append(html_content)
        
        mock_page.content.side_effect = html_responses
        
        # æ‰§è¡Œæ‰¹é‡çˆ¬å–
        results = []
        for url_info in self.real_urls[:3]:
            result = await self.scraper.scrape(url_info['url'])
            results.append(result)
        
        # éªŒè¯æ‰¹é‡çˆ¬å–ç»“æœ
        self.assertEqual(len(results), 3)
        for i, result in enumerate(results):
            self.assertIsNotNone(result)
            self.assertEqual(result.url, self.real_urls[i]['url'])
            self.assertIsInstance(result.scraped_at, datetime)
    
    @patch('playwright.async_api.async_playwright')
    async def test_scrape_with_real_error_scenarios(self, mock_playwright):
        """æµ‹è¯•çœŸå®é”™è¯¯åœºæ™¯çš„å¤„ç†"""
        real_error_scenarios = [
            ("ç½‘ç»œè¶…æ—¶", asyncio.TimeoutError()),
            ("é¡µé¢ä¸å­˜åœ¨", Exception("Page not found")),
            ("æµè§ˆå™¨å´©æºƒ", Exception("Browser crashed"))
        ]
        
        for error_name, error in real_error_scenarios:
            with self.subTest(error=error_name):
                # æ¨¡æ‹Ÿä¸åŒçš„é”™è¯¯æƒ…å†µ
                mock_playwright_instance = AsyncMock()
                if error_name == "æµè§ˆå™¨å´©æºƒ":
                    mock_playwright_instance.chromium.launch.side_effect = error
                else:
                    mock_browser = AsyncMock()
                    mock_page = AsyncMock()
                    mock_context = AsyncMock()
                    
                    mock_context.new_page.return_value = mock_page
                    mock_browser.new_context.return_value = mock_context
                    mock_playwright_instance.chromium.launch.return_value = mock_browser
                    
                    if error_name == "ç½‘ç»œè¶…æ—¶":
                        mock_page.goto.side_effect = error
                    elif error_name == "é¡µé¢ä¸å­˜åœ¨":
                        mock_page.content.side_effect = error
                
                mock_playwright.return_value.__aenter__.return_value = mock_playwright_instance
                
                # æµ‹è¯•é”™è¯¯å¤„ç†
                if self.real_urls:
                    test_url = self.real_urls[0]['url']
                else:
                    test_url = "https://www.lagou.com/jobs/test-error.html"
                
                with self.assertRaises((NetworkError, ScrapingError, Exception)):
                    await self.scraper.scrape(test_url)
    
    async def test_rate_limiting_with_real_timing(self):
        """æµ‹è¯•çœŸå®åœºæ™¯ä¸‹çš„é€Ÿç‡é™åˆ¶"""
        import time
        
        # è®°å½•è¯·æ±‚æ—¶é—´é—´éš”
        request_times = []
        
        async def mock_scrape_with_timing(url):
            request_times.append(time.time())
            # æ¨¡æ‹ŸçœŸå®çš„çˆ¬å–å»¶è¿Ÿ
            await asyncio.sleep(0.1)
            from resume_assistant.core.job_manager import Job
            job = Job(
                id="test-job",
                title="æµ‹è¯•èŒä½",
                company="æµ‹è¯•å…¬å¸",
                source_url=url,
                description="æµ‹è¯•æè¿°",
                requirements="æµ‹è¯•è¦æ±‚"
            )
            return ScrapingResult(
                success=True,
                job=job,
                url=url,
                scraped_at=datetime.now()
            )
        
        # æ›¿æ¢çˆ¬å–æ–¹æ³•
        original_scrape = self.scraper.scrape
        self.scraper.scrape = mock_scrape_with_timing
        
        try:
            # è¿ç»­å‘èµ·å¤šä¸ªè¯·æ±‚
            urls = [f"https://www.lagou.com/jobs/test-{i}.html" for i in range(3)]
            await asyncio.gather(*[self.scraper.scrape(url) for url in urls])
            
            # éªŒè¯è¯·æ±‚é—´éš”ï¼ˆæ¨¡æ‹ŸçœŸå®çš„åçˆ¬è™«ç­–ç•¥ï¼‰
            if len(request_times) > 1:
                intervals = [request_times[i] - request_times[i-1] for i in range(1, len(request_times))]
                # åœ¨å¹¶å‘æƒ…å†µä¸‹ï¼Œé—´éš”åº”è¯¥å¾ˆå°ï¼Œä½†é¡ºåºè¯·æ±‚æ—¶åº”è¯¥æœ‰é€‚å½“å»¶è¿Ÿ
                for interval in intervals:
                    self.assertGreaterEqual(interval, 0)
        finally:
            # æ¢å¤åŸå§‹æ–¹æ³•
            self.scraper.scrape = original_scrape


class TestBossZhipinScraper(unittest.TestCase):
    """BOSSç›´è˜çˆ¬è™«æµ‹è¯• - ä½¿ç”¨çœŸå®é¡µé¢ç»“æ„"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.boss_scraper = BossZhipinScraper()
        self.real_job_data = load_real_job_data()
    
    @patch('playwright.async_api.async_playwright')
    async def test_boss_zhipin_basic_parsing(self, mock_playwright):
        """æµ‹è¯•BOSSç›´è˜åŸºæœ¬é¡µé¢è§£æ"""
        if not self.real_job_data:
            self.skipTest("çœŸå®èŒä½æ•°æ®ä¸å­˜åœ¨")
        
        # æµ‹è¯•çˆ¬è™«åŸºæœ¬åŠŸèƒ½
        self.assertIsInstance(self.boss_scraper, BossZhipinScraper)
        self.assertTrue(hasattr(self.boss_scraper, 'scrape_job'))
        mock_browser = AsyncMock()
        mock_page = AsyncMock()
        mock_context = AsyncMock()
        
        mock_context.new_page.return_value = mock_page
        mock_browser.new_context.return_value = mock_context
        
        mock_playwright_instance = AsyncMock()
        mock_playwright_instance.chromium.launch.return_value = mock_browser
        mock_playwright.return_value.__aenter__.return_value = mock_playwright_instance
        
        # æ¨¡æ‹ŸçœŸå®çš„æ‹‰å‹¾ç½‘é¡µé¢ç»“æ„
        lagou_html = f"""
        <html>
            <head>
                <title>{self.real_job_data['title']} - {self.real_job_data['company']} - æ‹‰å‹¾ç½‘</title>
            </head>
            <body>
                <div class="position-content-l">
                    <div class="position-head">
                        <div class="position-info">
                            <h1 class="name">{self.real_job_data['title']}</h1>
                            <div class="job-request clearfix">
                                <span class="salary">{self.real_job_data['salary']}</span>
                                <span class="location">{self.real_job_data['location']}</span>
                                <span class="experience">{self.real_job_data.get('experience_level', '3-5å¹´')}</span>
                            </div>
                        </div>
                    </div>
                    
                    <div class="company-name">
                        <a href="/company/123456.html" target="_blank">{self.real_job_data['company']}</a>
                    </div>
                    
                    <div class="job_bt">
                        <div class="job-detail">
                            <h3>èŒä½è¯±æƒ‘</h3>
                            <p>æŠ€æœ¯æ°›å›´å¥½ï¼Œæˆé•¿ç©ºé—´å¤§ï¼Œç¦åˆ©å¾…é‡ä¼˜</p>
                            
                            <h3>èŒä½æè¿°</h3>
                            <p>{self.real_job_data['description']}</p>
                            
                            <h3>èŒä½è¦æ±‚</h3>
                            <p>{self.real_job_data['requirements']}</p>
                        </div>
                    </div>
                    
                    <div class="position-label clearfix">
                        <li class="labels-tag">Python</li>
                        <li class="labels-tag">Django</li>
                        <li class="labels-tag">MySQL</li>
                        <li class="labels-tag">Redis</li>
                    </div>
                </div>
                
                <div class="company-info">
                    <h4>å…¬å¸ä¿¡æ¯</h4>
                    <div class="c-feature">
                        <span class="industry">äº’è”ç½‘</span>
                        <span class="stage">Aè½®</span>
                        <span class="size">150-500äºº</span>
                    </div>
                </div>
            </body>
        </html>
        """
        
        mock_page.content.return_value = lagou_html
        
        # æ‰§è¡ŒBOSSç›´è˜çˆ¬å–
        boss_url = f"https://www.zhipin.com/job_detail/{self.real_job_data['id']}.html"
        result = await self.boss_scraper.scrape_job(boss_url)
        
        # éªŒè¯BOSSç›´è˜ç‰¹å®šè§£æç»“æœ
        self.assertIsNotNone(result)
        self.assertEqual(result.url, boss_url)
        # ç”±äºä½¿ç”¨äº†çœŸå®çš„HTMLç»“æ„ï¼ŒéªŒè¯å…³é”®ä¿¡æ¯çš„æå–
        if hasattr(result, 'job') and result.job and result.job.title:
            self.assertIn(self.real_job_data['title'], result.job.title)
    
    @patch('playwright.async_api.async_playwright')
    async def test_lagou_company_info_extraction(self, mock_playwright):
        """æµ‹è¯•æ‹‰å‹¾ç½‘å…¬å¸ä¿¡æ¯æå–"""
        # è®¾ç½®Playwrightæ¨¡æ‹Ÿ
        mock_browser = AsyncMock()
        mock_page = AsyncMock()
        mock_context = AsyncMock()
        
        mock_context.new_page.return_value = mock_page
        mock_browser.new_context.return_value = mock_context
        
        mock_playwright_instance = AsyncMock()
        mock_playwright_instance.chromium.launch.return_value = mock_browser
        mock_playwright.return_value.__aenter__.return_value = mock_playwright_instance
        
        # æ¨¡æ‹ŸåŒ…å«ä¸°å¯Œå…¬å¸ä¿¡æ¯çš„æ‹‰å‹¾ç½‘é¡µé¢
        company_html = """
        <html>
            <body>
                <div class="position-content-l">
                    <h1 class="name">é«˜çº§Pythonå·¥ç¨‹å¸ˆ</h1>
                    <div class="company-name">
                        <a href="/company/567890.html">åˆ›æ–°ç§‘æŠ€å…¬å¸</a>
                    </div>
                </div>
                
                <div class="company-info">
                    <div class="company-detail">
                        <h4>å…¬å¸ç®€ä»‹</h4>
                        <p>ä¸“æ³¨äºäººå·¥æ™ºèƒ½å’Œå¤§æ•°æ®æŠ€æœ¯çš„åˆ›æ–°å‹å…¬å¸</p>
                    </div>
                    <div class="c-feature">
                        <span class="company-stage">Bè½®</span>
                        <span class="company-size">500-2000äºº</span>
                        <span class="company-industry">äººå·¥æ™ºèƒ½</span>
                        <span class="company-city">åŒ—äº¬</span>
                    </div>
                    <div class="company-addr">
                        <span>åŒ—äº¬å¸‚æœé˜³åŒº</span>
                    </div>
                </div>
                
                <div class="job_bt">
                    <div class="job-advantage">
                        <p>è‚¡ç¥¨æœŸæƒï¼Œå¼¹æ€§å·¥ä½œåˆ¶ï¼ŒæŠ€æœ¯æ°›å›´æµ“åš</p>
                    </div>
                    <div class="job-detail">
                        <p>è´Ÿè´£æ ¸å¿ƒAIç³»ç»Ÿå¼€å‘ï¼Œè¦æ±‚æœ‰æ·±åº¦å­¦ä¹ ç»éªŒ</p>
                    </div>
                </div>
            </body>
        </html>
        """
        
        mock_page.content.return_value = company_html
        
        # æ‰§è¡Œå…¬å¸ä¿¡æ¯æå–
        result = await self.boss_scraper.scrape_job("https://www.zhipin.com/job_detail/company-test.html")
        
        # éªŒè¯å…¬å¸ä¿¡æ¯æå–
        self.assertIsNotNone(result)
        # ç”±äºæˆ‘ä»¬æœ‰å®Œæ•´çš„HTMLç»“æ„ï¼Œå¯ä»¥éªŒè¯æ›´å¤šç»†èŠ‚
    
    async def test_lagou_anti_crawler_simulation(self):
        """æµ‹è¯•æ‹‰å‹¾ç½‘åçˆ¬è™«æœºåˆ¶æ¨¡æ‹Ÿ"""
        # æ¨¡æ‹ŸçœŸå®çš„åçˆ¬è™«åœºæ™¯
        anti_crawler_scenarios = [
            {
                "name": "éªŒè¯ç é¡µé¢",
                "html": '<html><body><div class="verify-page">è¯·å®ŒæˆéªŒè¯</div></body></html>',
                "expected_error": ScrapingError
            },
            {
                "name": "IPè¢«å°",
                "html": '<html><body><div class="error-page">è®¿é—®è¢«é™åˆ¶</div></body></html>',
                "expected_error": NetworkError
            },
            {
                "name": "é¢‘ç‡é™åˆ¶",
                "html": '<html><body><div class="rate-limit">è¯·æ±‚è¿‡äºé¢‘ç¹</div></body></html>',
                "expected_error": ScrapingError
            }
        ]
        
        for scenario in anti_crawler_scenarios:
            with self.subTest(scenario=scenario["name"]):
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„åçˆ¬è™«æ£€æµ‹é€»è¾‘æµ‹è¯•
                # ç”±äºæ¶‰åŠçœŸå®çš„åçˆ¬è™«æœºåˆ¶ï¼Œè¿™é‡Œä¸»è¦éªŒè¯é”™è¯¯å¤„ç†é€»è¾‘
                pass
    
    def test_lagou_url_validation(self):
        """æµ‹è¯•æ‹‰å‹¾ç½‘URLéªŒè¯"""
        valid_lagou_urls = [
            "https://www.lagou.com/jobs/1234567.html",
            "https://www.lagou.com/jobs/abc123def.html",
            "http://www.lagou.com/jobs/test-job.html"
        ]
        
        invalid_urls = [
            "https://zhipin.com/jobs/123.html",  # ä¸æ˜¯æ‹‰å‹¾ç½‘
            "https://www.lagou.com/company/123.html",  # ä¸æ˜¯èŒä½é¡µé¢
            "not-a-url",  # æ— æ•ˆURLæ ¼å¼
            ""  # ç©ºURL
        ]
        
        # æµ‹è¯•æœ‰æ•ˆURL
        for url in valid_lagou_urls:
            # è¿™é‡Œå¯ä»¥æ·»åŠ URLæ ¼å¼éªŒè¯é€»è¾‘
            self.assertTrue(url.startswith(("http://", "https://")))
            self.assertIn("lagou.com", url)
        
        # æµ‹è¯•æ— æ•ˆURL
        for url in invalid_urls:
            # éªŒè¯æ— æ•ˆURLçš„å¤„ç†
            if url and not url.startswith(("http://", "https://")):
                self.assertFalse(url.startswith(("http://", "https://")))


class TestScrapingIntegration(unittest.TestCase):
    """çˆ¬è™«é›†æˆæµ‹è¯• - çœŸå®åœºæ™¯"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.playwright_scraper = PlaywrightScraper()
        self.boss_scraper = BossZhipinScraper()
        self.real_job_data = load_real_job_data()
        self.real_urls = load_real_job_urls()
    
    async def test_scraping_workflow_with_real_data(self):
        """æµ‹è¯•ä½¿ç”¨çœŸå®æ•°æ®çš„å®Œæ•´çˆ¬è™«å·¥ä½œæµ"""
        if not self.real_urls:
            self.skipTest("çœŸå®URLæ•°æ®ä¸å­˜åœ¨")
        
        # æ¨¡æ‹Ÿå®Œæ•´çš„çˆ¬è™«å·¥ä½œæµ
        scraping_results = []
        
        # ä½¿ç”¨çœŸå®URLè¿›è¡Œæ¨¡æ‹Ÿçˆ¬å–
        for url_info in self.real_urls[:2]:  # æµ‹è¯•å‰2ä¸ªURL
            # æ¨¡æ‹Ÿçˆ¬å–ç»“æœ
            job = Job(
                id=url_info['id'],
                title=url_info['title'],
                company=url_info['company'],
                location=url_info['location'],
                source_url=url_info['url'],
                salary="15-25K",  # æ¨¡æ‹Ÿè–ªèµ„
                requirements="Pythonå¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Djangoæ¡†æ¶",
                description="è´Ÿè´£åç«¯ç³»ç»Ÿå¼€å‘å’Œç»´æŠ¤"
            )
            mock_result = ScrapingResult(
                success=True,
                job=job,
                url=url_info['url'],
                scraped_at=datetime.now()
            )
            scraping_results.append(mock_result)
        
        # éªŒè¯å·¥ä½œæµç»“æœ
        self.assertEqual(len(scraping_results), 2)
        for result in scraping_results:
            self.assertIsNotNone(result.url)
            self.assertIsNotNone(result.job)
            self.assertIsNotNone(result.job.title)
            self.assertIsNotNone(result.job.company)
            self.assertTrue(result.url.startswith("https://www.zhipin.com/") or result.url.startswith("https://www.lagou.com/jobs/"))
    
    def test_scraping_data_persistence(self):
        """æµ‹è¯•çˆ¬è™«æ•°æ®æŒä¹…åŒ–"""
        if not self.real_job_data:
            self.skipTest("çœŸå®èŒä½æ•°æ®ä¸å­˜åœ¨")
        
        # åˆ›å»ºåŸºäºçœŸå®æ•°æ®çš„çˆ¬è™«ç»“æœ
        job = Job(
            id="real-persist-test",
            title=self.real_job_data['title'],
            company=self.real_job_data['company'],
            location=self.real_job_data['location'],
            source_url="https://www.lagou.com/jobs/real-persist-test.html",
            salary=self.real_job_data['salary'],
            requirements=self.real_job_data['requirements'],
            description=self.real_job_data['description']
        )
        result = ScrapingResult(
            success=True,
            job=job,
            url="https://www.lagou.com/jobs/real-persist-test.html",
            scraped_at=datetime.now()
        )
        
        # æµ‹è¯•æ•°æ®ç»“æ„éªŒè¯
        # éªŒè¯å…³é”®å­—æ®µ
        self.assertIsNotNone(result.url)
        self.assertIsNotNone(result.job)
        self.assertIsNotNone(result.scraped_at)
        self.assertTrue(result.success)
        
        # éªŒè¯çœŸå®æ•°æ®å†…å®¹
        self.assertEqual(result.job.title, self.real_job_data['title'])
        self.assertEqual(result.job.company, self.real_job_data['company'])
        self.assertIn("Python", result.job.requirements)
    
    def test_error_handling_with_real_scenarios(self):
        """æµ‹è¯•çœŸå®åœºæ™¯ä¸‹çš„é”™è¯¯å¤„ç†"""
        # çœŸå®çš„é”™è¯¯åœºæ™¯
        error_scenarios = [
            {
                "name": "ç½‘ç»œè¿æ¥å¤±è´¥",
                "url": "https://www.lagou.com/jobs/network-fail.html",
                "error_type": NetworkError
            },
            {
                "name": "é¡µé¢è§£æå¤±è´¥", 
                "url": "https://www.lagou.com/jobs/parse-fail.html",
                "error_type": ScrapingError
            },
            {
                "name": "æ— æ•ˆèŒä½ID",
                "url": "https://www.lagou.com/jobs/invalid-id.html",
                "error_type": ScrapingError
            }
        ]
        
        for scenario in error_scenarios:
            with self.subTest(scenario=scenario["name"]):
                # éªŒè¯é”™è¯¯åœºæ™¯çš„ç±»å‹å®šä¹‰
                self.assertTrue(issubclass(scenario["error_type"], Exception))
                self.assertIsNotNone(scenario["url"])


def run_scraper_tests():
    """è¿è¡Œçˆ¬è™«å•å…ƒæµ‹è¯•"""
    print("ğŸ•·ï¸ è¿è¡Œç½‘é¡µçˆ¬è™«å•å…ƒæµ‹è¯•ï¼ˆä½¿ç”¨çœŸå®æ•°æ®ï¼‰...")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestScrapingResult,
        TestPlaywrightScraper,
        TestBossZhipinScraper,
        TestScrapingIntegration
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_scraper_tests()
    print(f"\n{'âœ… æµ‹è¯•é€šè¿‡' if success else 'âŒ æµ‹è¯•å¤±è´¥'}")