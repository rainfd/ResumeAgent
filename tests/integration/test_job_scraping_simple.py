"""ç®€åŒ–çš„èŒä½çˆ¬å–å·¥ä½œæµé›†æˆæµ‹è¯•

æµ‹è¯•ä»è¾“å…¥BOSSç›´è˜ç½‘å€åˆ°æŸ¥çœ‹èŒä½è¯¦æƒ…çš„å®Œæ•´ç”¨æˆ·æµç¨‹
"""

import unittest
import asyncio
import sys
import json
from pathlib import Path
from unittest.mock import patch, MagicMock, AsyncMock
from datetime import datetime
import time
import pytest

# æ·»åŠ srcè·¯å¾„åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from resume_assistant.core.job_manager import JobManager, Job
from resume_assistant.core.scraper import BossZhipinScraper, ScrapingResult
from resume_assistant.web.session_manager import SessionManager
from resume_assistant.utils.errors import NetworkError, ParseError as ScrapingError, ResumeAssistantError


class TestJobScrapingWorkflowSimple(unittest.TestCase):
    """ç®€åŒ–çš„èŒä½çˆ¬å–å·¥ä½œæµé›†æˆæµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        # åˆå§‹åŒ–ç»„ä»¶
        self.job_manager = JobManager()
        self.session_manager = SessionManager()
        
        # æµ‹è¯•ç”¨çš„BOSSç›´è˜URL
        self.test_boss_urls = [
            "https://www.zhipin.com/job_detail/12345.html",
            "https://www.zhipin.com/job_detail/python-backend-67890.html",
            "https://www.zhipin.com/web/geek/job/abcd1234efgh5678.html"
        ]
        
        # æ¨¡æ‹Ÿçš„èŒä½æ•°æ®
        self.mock_job_data = {
            'id': 'scraped-job-001',
            'title': 'Pythonåç«¯å¼€å‘å·¥ç¨‹å¸ˆ',
            'company': 'BOSSç›´è˜ç§‘æŠ€æœ‰é™å…¬å¸',
            'location': 'åŒ—äº¬Â·æœé˜³åŒº',
            'salary': '18-30KÂ·14è–ª',
            'experience_level': '3-5å¹´',
            'education_level': 'æœ¬ç§‘',
            'description': '''
èŒä½æè¿°ï¼š
1. è´Ÿè´£å…¬å¸æ ¸å¿ƒä¸šåŠ¡ç³»ç»Ÿçš„åç«¯å¼€å‘å’Œç»´æŠ¤
2. å‚ä¸ç³»ç»Ÿæ¶æ„è®¾è®¡ï¼Œä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½
3. é…åˆå‰ç«¯å·¥ç¨‹å¸ˆå®Œæˆäº§å“åŠŸèƒ½å¼€å‘
4. å‚ä¸ä»£ç å®¡æŸ¥ï¼Œä¿è¯ä»£ç è´¨é‡

æŠ€æœ¯è¦æ±‚ï¼š
- ç†Ÿç»ƒæŒæ¡Pythonè¯­è¨€ï¼Œæœ‰Django/Flaskæ¡†æ¶ç»éªŒ
- ç†Ÿæ‚‰MySQLã€Redisç­‰æ•°æ®åº“
- äº†è§£å¾®æœåŠ¡æ¶æ„ï¼Œæœ‰å®¹å™¨åŒ–éƒ¨ç½²ç»éªŒ
- å…·å¤‡è‰¯å¥½çš„ä»£ç ä¹ æƒ¯å’Œæ–‡æ¡£æ„è¯†
            ''',
            'requirements': 'Python, Django, Flask, MySQL, Redis, å¾®æœåŠ¡, Docker',
            'company_info': {
                'size': '500-1000äºº',
                'stage': 'Dè½®åŠä»¥ä¸Š',
                'industry': 'äº’è”ç½‘'
            },
            'source_url': 'https://www.zhipin.com/job_detail/12345.html',
            'tags': ['Python', 'åç«¯å¼€å‘', 'äº’è”ç½‘'],
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat()
        }
    
    @patch('resume_assistant.core.scraper.BossZhipinScraper.scrape_job')
    def test_job_scraping_manager_workflow(self, mock_scrape_job):
        """æµ‹è¯•èŒä½çˆ¬å–ç®¡ç†å™¨å·¥ä½œæµ"""
        print("\nğŸ§ª å¼€å§‹æµ‹è¯•èŒä½çˆ¬å–ç®¡ç†å™¨å·¥ä½œæµ...")
        
        # 1. æ¨¡æ‹Ÿçˆ¬è™«è¿”å›ç»“æœ
        mock_job = Job(
            id=self.mock_job_data['id'],
            title=self.mock_job_data['title'],
            company=self.mock_job_data['company'],
            description=self.mock_job_data['description'],
            requirements=self.mock_job_data['requirements'],
            location=self.mock_job_data['location'],
            salary=self.mock_job_data['salary'],
            source_url=self.mock_job_data['source_url']
        )
        
        mock_scraping_result = ScrapingResult(
            success=True,
            job=mock_job,
            url=self.test_boss_urls[0],
            scraped_at=datetime.now()
        )
        mock_scrape_job.return_value = mock_scraping_result
        
        # 2. æµ‹è¯•é€šè¿‡JobManagerçˆ¬å–èŒä½
        print("ğŸ•·ï¸ æ­¥éª¤1ï¼šé€šè¿‡JobManagerçˆ¬å–èŒä½")
        
        # ä½¿ç”¨patchæ¥æ¨¡æ‹Ÿscrape_job_from_urlæ–¹æ³•
        with patch.object(self.job_manager, 'scrape_job_from_url') as mock_scrape:
            mock_scrape.return_value = mock_job
            
            # æ‰§è¡Œçˆ¬å–
            result = self.job_manager.scrape_job_from_url(self.test_boss_urls[0])
            
            # éªŒè¯ç»“æœ
            self.assertIsNotNone(result, "çˆ¬å–ç»“æœä¸åº”è¯¥ä¸ºç©º")
            self.assertEqual(result.title, self.mock_job_data['title'])
            self.assertEqual(result.company, self.mock_job_data['company'])
            self.assertEqual(result.source_url, self.test_boss_urls[0])
            
            print(f"  âœ“ æˆåŠŸçˆ¬å–èŒä½: {result.title}")
        
        # 3. æµ‹è¯•èŒä½ä¿¡æ¯éªŒè¯
        print("ğŸ“Š æ­¥éª¤2ï¼šéªŒè¯èŒä½ä¿¡æ¯å®Œæ•´æ€§")
        
        required_fields = ['id', 'title', 'company', 'location', 'salary', 'description']
        for field in required_fields:
            self.assertTrue(hasattr(result, field), f"èŒä½åº”è¯¥æœ‰{field}å­—æ®µ")
            value = getattr(result, field)
            self.assertIsNotNone(value, f"{field}ä¸åº”è¯¥ä¸ºç©º")
            print(f"  âœ“ {field}: {str(value)[:50]}...")
        
        print("âœ… èŒä½çˆ¬å–ç®¡ç†å™¨å·¥ä½œæµæµ‹è¯•é€šè¿‡")
    
    def test_boss_url_validation(self):
        """æµ‹è¯•BOSSç›´è˜URLéªŒè¯"""
        print("\nğŸ§ª æµ‹è¯•BOSSç›´è˜URLéªŒè¯...")
        
        def validate_boss_url(url: str) -> bool:
            """éªŒè¯BOSSç›´è˜URL"""
            if not url:
                return False
            if not url.startswith(('http://', 'https://')):
                return False
            if 'zhipin.com' not in url:
                return False
            return True
        
        # æµ‹è¯•æœ‰æ•ˆçš„BOSSç›´è˜URL
        valid_urls = [
            "https://www.zhipin.com/job_detail/12345.html",
            "https://www.zhipin.com/web/geek/job/abcd1234.html",
            "http://www.zhipin.com/job_detail/python-dev.html"
        ]
        
        # æµ‹è¯•æ— æ•ˆURL
        invalid_urls = [
            "https://www.lagou.com/jobs/12345.html",  # æ‹‰å‹¾ç½‘
            "https://jobs.51job.com/beijing/123456.html",  # 51job
            "not-a-url",  # æ— æ•ˆæ ¼å¼
            "javascript:alert(1)",  # æ¶æ„URL
            ""  # ç©ºURL
        ]
        
        for url in valid_urls:
            self.assertTrue(validate_boss_url(url), f"åº”è¯¥æ˜¯æœ‰æ•ˆçš„BOSSç›´è˜URL: {url}")
            print(f"  âœ“ éªŒè¯æœ‰æ•ˆURL: {url}")
        
        for url in invalid_urls:
            self.assertFalse(validate_boss_url(url), f"åº”è¯¥æ˜¯æ— æ•ˆURL: {url}")
            print(f"  âœ“ éªŒè¯æ— æ•ˆURL: {url}")
        
        print("âœ… URLéªŒè¯æµ‹è¯•é€šè¿‡")
    
    @patch('resume_assistant.core.scraper.BossZhipinScraper.scrape_job')
    def test_scraping_error_handling(self, mock_scrape_job):
        """æµ‹è¯•çˆ¬å–é”™è¯¯å¤„ç†"""
        print("\nğŸ§ª æµ‹è¯•çˆ¬å–é”™è¯¯å¤„ç†...")
        
        # æµ‹è¯•ä¸åŒçš„é”™è¯¯åœºæ™¯
        error_scenarios = [
            {
                'name': 'ç½‘ç»œè¶…æ—¶é”™è¯¯',
                'exception': NetworkError("è¿æ¥è¶…æ—¶"),
                'expected_type': NetworkError
            },
            {
                'name': 'é¡µé¢è§£æé”™è¯¯', 
                'exception': ScrapingError("é¡µé¢ç»“æ„å˜åŒ–"),
                'expected_type': ScrapingError
            },
            {
                'name': 'é€šç”¨å¼‚å¸¸',
                'exception': Exception("æœªçŸ¥é”™è¯¯"),
                'expected_type': Exception
            }
        ]
        
        for scenario in error_scenarios:
            print(f"  ğŸ” æµ‹è¯•åœºæ™¯: {scenario['name']}")
            
            # æ¨¡æ‹Ÿé”™è¯¯
            mock_scrape_job.side_effect = scenario['exception']
            
            # ä½¿ç”¨patchæ¥æ¨¡æ‹ŸJobManagerçš„é”™è¯¯å¤„ç†
            with patch.object(self.job_manager, 'scrape_job_from_url') as mock_scrape:
                mock_scrape.side_effect = scenario['exception']
                
                # éªŒè¯å¼‚å¸¸å¤„ç†
                with self.assertRaises(scenario['expected_type']):
                    self.job_manager.scrape_job_from_url(self.test_boss_urls[0])
            
            print(f"    âœ“ {scenario['name']}å¤„ç†æ­£å¸¸")
        
        print("âœ… é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")
    
    def test_job_data_structure(self):
        """æµ‹è¯•èŒä½æ•°æ®ç»“æ„"""
        print("\nğŸ§ª æµ‹è¯•èŒä½æ•°æ®ç»“æ„...")
        
        # åˆ›å»ºJobå¯¹è±¡
        job = Job(
            id=self.mock_job_data['id'],
            title=self.mock_job_data['title'],
            company=self.mock_job_data['company'],
            description=self.mock_job_data['description'],
            requirements=self.mock_job_data['requirements'],
            location=self.mock_job_data['location'],
            salary=self.mock_job_data['salary'],
            source_url=self.mock_job_data['source_url']
        )
        
        # éªŒè¯å¿…è¦å­—æ®µ
        self.assertIsNotNone(job.id, "èŒä½IDä¸èƒ½ä¸ºç©º")
        self.assertIsNotNone(job.title, "èŒä½æ ‡é¢˜ä¸èƒ½ä¸ºç©º")
        self.assertIsNotNone(job.company, "å…¬å¸åç§°ä¸èƒ½ä¸ºç©º")
        self.assertIsNotNone(job.description, "èŒä½æè¿°ä¸èƒ½ä¸ºç©º")
        
        # éªŒè¯å­—æ®µç±»å‹
        self.assertIsInstance(job.title, str, "èŒä½æ ‡é¢˜åº”è¯¥æ˜¯å­—ç¬¦ä¸²")
        self.assertIsInstance(job.company, str, "å…¬å¸åç§°åº”è¯¥æ˜¯å­—ç¬¦ä¸²")
        
        # éªŒè¯URLæ ¼å¼
        self.assertTrue(job.source_url.startswith(('http://', 'https://')), "æ¥æºURLåº”è¯¥æœ‰åè®®")
        
        print(f"  âœ“ èŒä½ID: {job.id}")
        print(f"  âœ“ èŒä½æ ‡é¢˜: {job.title}")
        print(f"  âœ“ å…¬å¸åç§°: {job.company}")
        print(f"  âœ“ å·¥ä½œåœ°ç‚¹: {job.location}")
        print(f"  âœ“ è–ªèµ„èŒƒå›´: {job.salary}")
        
        print("âœ… èŒä½æ•°æ®ç»“æ„æµ‹è¯•é€šè¿‡")
    
    def test_scraping_result_structure(self):
        """æµ‹è¯•çˆ¬å–ç»“æœç»“æ„"""
        print("\nğŸ§ª æµ‹è¯•çˆ¬å–ç»“æœç»“æ„...")
        
        # åˆ›å»ºJobå¯¹è±¡
        job = Job(
            id=self.mock_job_data['id'],
            title=self.mock_job_data['title'],
            company=self.mock_job_data['company'],
            description=self.mock_job_data['description'],
            requirements=self.mock_job_data['requirements'],
            location=self.mock_job_data['location'],
            salary=self.mock_job_data['salary'],
            source_url=self.mock_job_data['source_url']
        )
        
        # åˆ›å»ºScrapingResultå¯¹è±¡
        result = ScrapingResult(
            success=True,
            job=job,
            url=self.test_boss_urls[0],
            scraped_at=datetime.now()
        )
        
        # éªŒè¯ScrapingResultç»“æ„
        self.assertIsInstance(result.success, bool, "successåº”è¯¥æ˜¯å¸ƒå°”å€¼")
        self.assertTrue(result.success, "çˆ¬å–åº”è¯¥æˆåŠŸ")
        self.assertIsInstance(result.job, Job, "jobåº”è¯¥æ˜¯Jobå¯¹è±¡")
        self.assertEqual(result.url, self.test_boss_urls[0], "URLåº”è¯¥åŒ¹é…")
        self.assertIsInstance(result.scraped_at, datetime, "scraped_atåº”è¯¥æ˜¯datetimeå¯¹è±¡")
        
        print(f"  âœ“ çˆ¬å–çŠ¶æ€: {result.success}")
        print(f"  âœ“ èŒä½å¯¹è±¡: {type(result.job).__name__}")
        print(f"  âœ“ çˆ¬å–URL: {result.url}")
        print(f"  âœ“ çˆ¬å–æ—¶é—´: {result.scraped_at}")
        
        print("âœ… çˆ¬å–ç»“æœç»“æ„æµ‹è¯•é€šè¿‡")
    
    def test_performance_timing(self):
        """æµ‹è¯•æ€§èƒ½æ—¶é—´"""
        print("\nğŸ§ª æµ‹è¯•æ€§èƒ½æ—¶é—´...")
        
        # æµ‹é‡Jobå¯¹è±¡åˆ›å»ºæ—¶é—´
        start_time = time.time()
        
        for i in range(100):
            job = Job(
                id=f"job-{i:03d}",
                title=f"Pythonå¼€å‘å·¥ç¨‹å¸ˆ {i+1}",
                company=f"ç§‘æŠ€å…¬å¸{chr(65 + i % 5)}",
                description="èŒä½æè¿°å†…å®¹",
                requirements="Python, Django, MySQL",
                location="åŒ—äº¬",
                salary="15k-25k",
                source_url=f"https://www.zhipin.com/job_detail/{i}.html"
            )
        
        creation_time = time.time() - start_time
        
        print(f"  â±ï¸ åˆ›å»º100ä¸ªJobå¯¹è±¡è€—æ—¶: {creation_time:.3f}ç§’")
        
        # éªŒè¯æ€§èƒ½åœ¨åˆç†èŒƒå›´å†…
        self.assertLess(creation_time, 1.0, "Jobå¯¹è±¡åˆ›å»ºæ—¶é—´åº”è¯¥åœ¨1ç§’å†…")
        
        print("âœ… æ€§èƒ½æµ‹è¯•é€šè¿‡")


def run_job_scraping_simple_tests():
    """è¿è¡Œç®€åŒ–çš„èŒä½çˆ¬å–å·¥ä½œæµæµ‹è¯•"""
    print("ğŸ§ª è¿è¡Œç®€åŒ–çš„èŒä½çˆ¬å–å·¥ä½œæµé›†æˆæµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•æ–¹æ³•
    test_methods = [
        'test_job_scraping_manager_workflow',
        'test_boss_url_validation', 
        'test_scraping_error_handling',
        'test_job_data_structure',
        'test_scraping_result_structure',
        'test_performance_timing'
    ]
    
    for method_name in test_methods:
        test_suite.addTest(TestJobScrapingWorkflowSimple(method_name))
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
    result = runner.run(test_suite)
    
    return result.wasSuccessful()


if __name__ == "__main__":
    success = run_job_scraping_simple_tests()
    print(f"\n{'âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡' if success else 'âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥'}")